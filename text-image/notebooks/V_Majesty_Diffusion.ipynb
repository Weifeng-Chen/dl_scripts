{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwMUyt9LHG1"
      },
      "source": [
        "# V-Majesty Diffusion v1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skKJ4t753JOb"
      },
      "source": [
        "### Formerly known as Princess Generator ver. Victoria\n",
        "##### Access our [Majestic Guide](https://multimodal.art/majesty-diffusion) (_under construction_), our [GitHub](https://github.com/multimodalart/majesty-diffusion), join our community on [Discord](https://discord.gg/yNBtQBEDfZ) or reach out via [@multimodalart on Twitter](https://twitter.com/multimodalart)\n",
        "\\\n",
        " \n",
        "---\n",
        "\\\n",
        " \n",
        " \n",
        "#### CLIP Guided V-Diffusion by [dango233](https://github.com/Dango233/) and [apolinario (@multimodalart)](https://twitter.com/multimodalart).\n",
        "This notebook runs Dango233 edits of [Katherine Crowson](https://twitter.com/RiversHaveWings)'s v-objective guided diffusion. multimodalart added savable settings, MMC and assembled the Colab. Some functions and methods are from various code masters (nsheppard, DanielRussRuss and others)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atwVescFB1-p"
      },
      "source": [
        "## Save model and outputs on Google Drive? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AHukAUBzBxZf"
      },
      "outputs": [],
      "source": [
        "#@markdown Enable saving outputs to Google Drive to save your creations at AI/models\n",
        "save_outputs_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Enable saving models to Google Drive to avoid downloading the model every Colab instance\n",
        "save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_outputs_to_google_drive or save_models_to_google_drive:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "      drive.mount('/content/gdrive')\n",
        "    except:\n",
        "      save_outputs_to_google_drive = False\n",
        "      save_models_to_google_drive = False\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/AI/models\" if save_models_to_google_drive else \"/content/\"\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/v-majesty-diffusion\" if save_outputs_to_google_drive else \"/content/outputs\"\n",
        "!mkdir -p $model_path\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Model will be stored at {model_path}\")\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n",
        "\n",
        "#If you want to run it locally change it to true\n",
        "is_local = False\n",
        "skip_installs = False\n",
        "if(is_local):\n",
        "  model_path = \"/choose/your/local/model/path\"\n",
        "  outputs_path = \"/choose/your/local/outputs/path\"\n",
        "  skip_installs = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rybdftj4B9G_"
      },
      "source": [
        "## Setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xJ0J65iQCCfR"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "if(not skip_installs):\n",
        "    import subprocess\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
        "    if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
        "        downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(\".\")\n",
        "    !git clone https://github.com/crowsonkb/v-diffusion-pytorch\n",
        "    !git clone https://github.com/crowsonkb/guided-diffusion\n",
        "    !git clone https://github.com/multimodalart/majesty-diffusion\n",
        "    !git lfs clone https://github.com/LAION-AI/aesthetic-predictor\n",
        "    sys.path.append('./guided-diffusion')\n",
        "    !pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "    !pip install resize-right\n",
        "    !pip install lpips\n",
        "    !pip install piq\n",
        "    !pip install pytorch_lit\n",
        "    !pip install fairscale\n",
        "    from subprocess import Popen, PIPE\n",
        "    try:\n",
        "        import mmc\n",
        "    except:\n",
        "        # install mmc\n",
        "        !git clone https://github.com/apolinario/Multi-Modal-Comparators --branch gradient_checkpointing\n",
        "        !pip install poetry\n",
        "        !cd Multi-Modal-Comparators; poetry build\n",
        "        !cd Multi-Modal-Comparators; pip install dist/mmc*.whl\n",
        "        \n",
        "        # optional final step:\n",
        "        #poe napm_installs\n",
        "        !python Multi-Modal-Comparators/src/mmc/napm_installs/__init__.py\n",
        "    # suppress mmc warmup outputs\n",
        "    import mmc.loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aJms5ukdDXB9"
      },
      "outputs": [],
      "source": [
        "#@title Download models\n",
        "import os\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/secondary_model_imagenet_2.pth\"):\n",
        "  print(\"Using secondary model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/secondary_model_imagenet_2.pth https://the-eye.eu/public/AI/models/v-diffusion/secondary_model_imagenet_2.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/ava_vit_l_14_336_linear.pth\"):\n",
        "  print(\"Using ViT-L/14@336px aesthetic model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/ava_vit_l_14_336_linear.pth https://multimodal.art/models/ava_vit_l_14_336_linear.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/sa_0_4_vit_l_14_linear.pth\"):\n",
        "  print(\"Using ViT-L/14 aesthetic model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/sa_0_4_vit_l_14_linear.pth https://multimodal.art/models/sa_0_4_vit_l_14_linear.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/ava_vit_l_14_linear.pth\"):\n",
        "  print(\"Using ViT-L/14 aesthetic model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/ava_vit_l_14_linear.pth https://multimodal.art/models/ava_vit_l_14_linear.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/ava_vit_b_16_linear.pth\"):\n",
        "  print(\"Using ViT-B/16 aesthetic model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/ava_vit_b_16_linear.pth http://batbot.tv/ai/models/v-diffusion/ava_vit_b_16_linear.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/sa_0_4_vit_b_32_linear.pth\"):\n",
        "  print(\"Using ViT-B/32 aesthetic model from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/sa_0_4_vit_b_32_linear.pth https://multimodal.art/models/sa_0_4_vit_b_32_linear.pth\n",
        "\n",
        "if os.path.isfile(f\"{model_path}/openimages_512x_png_embed224.npz\"):\n",
        "  print(\"Using openimages png from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/openimages_512x_png_embed224.npz https://github.com/nshepperd/jax-guided-diffusion/raw/8437b4d390fcc6b57b89cedcbaf1629993c09d03/data/openimages_512x_png_embed224.npz\n",
        "if os.path.isfile(f\"{model_path}/imagenet_512x_jpg_embed224.npz\"):\n",
        "  print(\"Using imagenet antijpeg from Google Drive\")\n",
        "else:\n",
        "  !wget -O $model_path/imagenet_512x_jpg_embed224.npz https://github.com/nshepperd/jax-guided-diffusion/raw/8437b4d390fcc6b57b89cedcbaf1629993c09d03/data/imagenet_512x_jpg_embed224.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JmbrcrhpBPC6"
      },
      "outputs": [],
      "source": [
        "#@title Import stuff\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from piq import brisque\n",
        "from itertools import product\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "from numpy import nan\n",
        "sys.path.append('.')\n",
        "#sys.path.append('./CLIP')\n",
        "sys.path.append('v-diffusion-pytorch')\n",
        "sys.path.append('./ResizeRight/')\n",
        "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
        "from resize_right import resize\n",
        "\n",
        "import clip\n",
        "from diffusion import sampling, get_model, get_models, utils\n",
        "from pytorch_lit import LitModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YHOj78Yvx8jP"
      },
      "outputs": [],
      "source": [
        "#@title Define Necessary functions\n",
        "# Define necessary functions\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "def divide_chunks(l, n):\n",
        "      \n",
        "    # looping till length l\n",
        "    for i in range(0, len(l), n): \n",
        "        yield l[i:i + n]\n",
        "        \n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://') or prompt.startswith(\"E:\") or prompt.startswith(\"C:\") or prompt.startswith(\"D:\"):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size,\n",
        "                 Overview=4, \n",
        "                 WholeCrop = 0, WC_Allowance = 10, WC_Grey_P=0.2,\n",
        "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.Overview = Overview\n",
        "        self.WholeCrop= WholeCrop\n",
        "        self.WC_Allowance = WC_Allowance\n",
        "        self.WC_Grey_P = WC_Grey_P\n",
        "        self.InnerCrop = InnerCrop\n",
        "        self.IC_Size_Pow = IC_Size_Pow\n",
        "        self.IC_Grey_P = IC_Grey_P\n",
        "        self.augs = T.Compose([\n",
        "            #T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=0, \n",
        "                           translate=(0.05, 0.05), \n",
        "                           #scale=(0.9,0.95),\n",
        "                           fill=-1,  interpolation = T.InterpolationMode.BILINEAR, ),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            #T.RandomPerspective(p=1, interpolation = T.InterpolationMode.BILINEAR, fill=-1,distortion_scale=0.2),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.1),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        gray = transforms.Grayscale(3)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        l_size = max(sideX, sideY)\n",
        "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
        "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
        "        pad_input = F.pad(input,((sideY-max_size)//2+round(max_size*0.055),(sideY-max_size)//2+round(max_size*0.055),(sideX-max_size)//2+round(max_size*0.055),(sideX-max_size)//2+round(max_size*0.055)), **padargs)\n",
        "        cutouts_list = []\n",
        "        \n",
        "        if self.Overview>0:\n",
        "            cutouts = []\n",
        "            cutout = resize(pad_input, out_shape=output_shape)\n",
        "            if self.Overview in [1,2,4]:\n",
        "                if self.Overview>=2:\n",
        "                    cutout=torch.cat((cutout,gray(cutout)))\n",
        "                if self.Overview==4:\n",
        "                    cutout = torch.cat((cutout, TF.hflip(cutout)))\n",
        "            else:\n",
        "                output_shape_all = list(output_shape)\n",
        "                output_shape_all[0]=self.Overview\n",
        "                cutout = resize(pad_input, out_shape=output_shape_all)\n",
        "                if aug: cutout=self.augs(cutout)\n",
        "            cutouts_list.append(cutout)\n",
        "            \n",
        "        if self.InnerCrop >0:\n",
        "            cutouts=[]\n",
        "            for i in range(self.InnerCrop):\n",
        "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
        "                    cutout = gray(cutout)\n",
        "                cutout = resize(cutout, out_shape=output_shape)\n",
        "                cutouts.append(cutout)\n",
        "            if cutout_debug:\n",
        "                TF.to_pil_image(cutouts[-1].add(1).div(2).clamp(0, 1).squeeze(0)).save(\"content/diff/cutouts/cutout_InnerCrop.jpg\",quality=99)\n",
        "            cutouts_tensor = torch.cat(cutouts)\n",
        "            cutouts=[]\n",
        "            cutouts_list.append(cutouts_tensor)\n",
        "        cutouts=torch.cat(cutouts_list)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input, range_min, range_max):\n",
        "    return (input - input.clamp(range_min,range_max)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "def symmetric_loss(x):\n",
        "    w = x.shape[3]\n",
        "    diff = (x - torch.flip(x,[3])).square().mean().sqrt()/(x.shape[2]*x.shape[3]/1e4)\n",
        "    return(diff)\n",
        "def displayImage(image):\n",
        "  # image = unnormalize_image(image)\n",
        "  size = image.size()\n",
        "\n",
        "  width = size[0] * size[3] + (size[0]-1) * 4\n",
        "  image_row = torch.zeros( size=(3, size[2], width), dtype=torch.uint8 )\n",
        "\n",
        "  nw = 0\n",
        "  for n in range(size[0]):\n",
        "    image_row[:,:,nw:nw+size[3]] = (image[n,:].clamp(0, 1) * 255).to(torch.uint8)\n",
        "    nw += size[3] + 4\n",
        "\n",
        "  jpeg_data = torch.ops.image.encode_png(image_row.cpu(), 6)\n",
        "  image = display.Image(bytes(jpeg_data))\n",
        "  display.display( image )\n",
        "\n",
        "def unitwise_norm(x):\n",
        "    if len(x.squeeze().shape) <= 1:\n",
        "        dim = None\n",
        "        keepdim = False\n",
        "    elif len(x.shape) in (2, 3):\n",
        "        dim = 1\n",
        "        keepdim = True\n",
        "    elif len(x.shape) == 4:\n",
        "        dim = (1, 2, 3)\n",
        "        keepdim = True\n",
        "    else:\n",
        "        raise ValueError(f'got a parameter with shape not in (1, 2, 3, 4) {x}')\n",
        "    return x.norm(dim = dim, keepdim = keepdim, p = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vuD4LUi9BKTO"
      },
      "outputs": [],
      "source": [
        "#@title Define the secondary diffusion model\n",
        "# Define the secondary diffusion model\n",
        "\n",
        "def append_dims(x, n):\n",
        "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
        "\n",
        "\n",
        "def expand_to_planes(x, shape):\n",
        "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
        "\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "\n",
        "def t_to_alpha_sigma(t):\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = nn.AvgPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        " \n",
        "secondary_model = SecondaryDiffusionImageNet2()\n",
        "secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
        "secondary_model = secondary_model.eval().requires_grad_(False).to(\"cuda\") \n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32,16,8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing':\"16,48,72\", #24,48,6'8，16，64 8,12,16,32',#'16,24,32,64',  # Modify this value to decrease the number of                                 # timesteps.\n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': True\n",
        "})\n",
        "\n",
        "def wrapped_openai(x, t):\n",
        "    x = x\n",
        "    t = t\n",
        "    return openai(x, t * 1000)[:, :3]\n",
        "\n",
        "def cfg_model_fn(x, t):\n",
        "    \"\"\"The CFG wrapper function.\"\"\"\n",
        "    n = x.shape[0]\n",
        "    x_in = x.repeat([target_embeds[\"ViT-B-16--openai\"].shape[0]+1, 1, 1, 1])\n",
        "    t_in = t.repeat([target_embeds[\"ViT-B-16--openai\"].shape[0]+1])\n",
        "    clip_embed_repeat = target_embeds[\"ViT-B-16--openai\"].repeat([n, 1])\n",
        "    clip_embed_in = torch.cat([torch.zeros_like(clip_embed_repeat[0].unsqueeze(0)), clip_embed_repeat])\n",
        "    v_all = model[\"cc12m_1_cfg\"](x_in, t_in, clip_embed_in)\n",
        "    v_uncond = v_all[0].unsqueeze(0)\n",
        "    v_cond = v_all[1:].mean(0).squeeze(0)\n",
        "    v = v_uncond + (v_cond - v_uncond) * cfg_scale\n",
        "    v = v.mean(0).squeeze(0)\n",
        "    return v\n",
        "\n",
        "has_loaded_custom = False\n",
        "#model[\"cc12m_1_cfg\"]=cfg_model_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4cJY1aEXrqk"
      },
      "source": [
        "## Initial Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fpbody2NCR7w",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Choose your diffusion models\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "\n",
        "model_list = []\n",
        "model = {}\n",
        "pace = []\n",
        "def load_diffusion_models(reload=True):\n",
        "  global model_list\n",
        "  global model\n",
        "  global pace\n",
        "  global openai\n",
        "  if(reload==True):\n",
        "    #@markdown <small>`imagenet_openimages` and `yfcc_2` work well with images > 256x256<small>\n",
        "    imagenet_openimages = True #@param {type:\"boolean\"}\n",
        "    yfcc_2 = False #@param {type:\"boolean\"}\n",
        "    #@markdown <small>The `cc12m_1` family of models require ViT-B/16 CLIP-Guidance, work best 256x256, but you can use yfcc2 or imagenet to upscale<small>\n",
        "    cc12m_1_cfg = False #@param {type:\"boolean\"}\n",
        "    cc12m_1 = False #@param {type:\"boolean\"}\n",
        "    wikiart_256 = False #@param {type:\"boolean\"}\n",
        "    nshep_danbooru = False #@param {type:\"boolean\"}\n",
        "    danbooru_128 = False #@param {type:\"boolean\"}\n",
        "    model_list = []\n",
        "    model = {}\n",
        "  else:\n",
        "    cc12m_1_cfg = False\n",
        "    cc12m_1 = False\n",
        "    yfcc_2 = False\n",
        "    imagenet_openimages = False\n",
        "    wikiart_256 = False\n",
        "    nshep_danbooru = False\n",
        "    danbooru_128 = False\n",
        "  pace = []\n",
        "  if(cc12m_1_cfg or 'cc12m_1_cfg' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/cc12m_1_cfg.pth\"):\n",
        "      print(\"Using cc12m_1_cfg from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/cc12m_1_cfg.pth https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1_cfg.pth\n",
        "    if 'cc12m_1_cfg' not in model_list:\n",
        "      model_list.append(\"cc12m_1_cfg\")\n",
        "  if(cc12m_1 or 'cc12m_1' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/cc12m_1.pth\"):\n",
        "      print(\"Using cc12m model saved from Google Drive\")\n",
        "    else:    \n",
        "        !wget -O $model_path/cc12m_1.pth https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1.pth\n",
        "    if 'cc12m_1' not in model_list:\n",
        "      model_list.append(\"cc12m_1\")\n",
        "  if(yfcc_2 or 'yfcc_2' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/yfcc_2.pth\"):\n",
        "      print(\"Using yfcc_2 from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/yfcc_2.pth https://the-eye.eu/public/AI/models/v-diffusion/yfcc_2.pth\n",
        "    if 'yfcc_2' not in model_list:\n",
        "      model_list.append(\"yfcc_2\")\n",
        "  if(imagenet_openimages or 'openimages' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/openimages.pth\"):\n",
        "      print(\"Using openimages from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/openimages.pth https://set.zlkj.in/models/diffusion/512x512_diffusion_uncond_openimages_epoch28_withfilter.pt\n",
        "    if 'openimages' not in model_list:\n",
        "      model_list.append(\"openimages\")\n",
        "  if(wikiart_256 or 'wikiart_256' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/wikiart_256.pth\"):\n",
        "      print(\"Using wikiart_256 model from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/wikiart_256.pth https://the-eye.eu/public/AI/models/v-diffusion/wikiart_256.pth\n",
        "    if 'wikiart_256' not in model_list:\n",
        "      model_list.append(\"wikiart_256\")\n",
        "  if(nshep_danbooru or 'nshep_danbooru' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/nshep_danbooru.pth\"):\n",
        "      print(\"Using danbooru model from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/nshep_danbooru.pth https://set.zlkj.in/models/diffusion/danbooru/cc12m-danbooru-adam-lr5-1645.pt\n",
        "    if 'nshep_danbooru' not in model_list:\n",
        "      model_list.append(\"nshep_danbooru\")\n",
        "  if(danbooru_128 or 'danbooru_128' in model_list):\n",
        "    if os.path.isfile(f\"{model_path}/danbooru_128.pth\"):\n",
        "      print(\"Using danbooru model from Google Drive\")\n",
        "    else:\n",
        "      !wget -O $model_path/danbooru_128.pth https://the-eye.eu/public/AI/models/v-diffusion/danbooru_128.pth\n",
        "    if 'danbooru_128' not in model_list:\n",
        "      model_list.append(\"danbooru_128\")\n",
        "\n",
        "  ##@markdown #### Use Pytorch Light Intefence Toolkit\n",
        "  ##@markdown #####(allow for bigger things, reduces VRAM usage, have to use cfg or secondary model if activated)\n",
        "  use_LIT = False \n",
        "  \n",
        "  if use_LIT:\n",
        "      for model_name in model_list:\n",
        "          checkpoint = f\"{model_path}/\"+model_name+\".pth\"\n",
        "          if model_name != \"openimages\":\n",
        "              if(model_name == 'nshep_danbooru'):\n",
        "                model[model_name] = get_model('cc12m_1')()\n",
        "              else:\n",
        "                model[model_name] = get_model(model_name)()\n",
        "              #model[model_name].load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
        "              #lmodel[model_name] = model[model_name].half()\n",
        "              model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
        "              model[model_name] = LitModule.from_params(\"models/\"+model_name,\n",
        "                                        lambda: model[model_name],\n",
        "                                        device=\"cuda\")\n",
        "          elif model_name == \"openimages\":\n",
        "              openai, diffusion = create_model_and_diffusion(**model_config)\n",
        "              openai.load_state_dict(torch.load(f\"{model_path}/openimages.pth\", map_location='cpu'))\n",
        "              openai.requires_grad_(False).eval().to(device)\n",
        "\n",
        "              for name, param in openai.named_parameters():\n",
        "                  if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "                      param.requires_grad_()\n",
        "              if model_config['use_fp16']:\n",
        "                  openai.convert_to_fp16()\n",
        "              openai = LitModule.from_params(\"models/openimages\",\n",
        "                                        lambda: openai,\n",
        "                                        device=\"cuda\")\n",
        "              model[\"openimages\"] = wrapped_openai\n",
        "  else:\n",
        "      for model_name in model_list:\n",
        "          checkpoint = f\"{model_path}/\"+model_name+\".pth\"\n",
        "          if model_name != \"openimages\":\n",
        "              if(model_name == 'nshep_danbooru'):\n",
        "                model[model_name] = get_model('cc12m_1')()\n",
        "              else:\n",
        "                model[model_name] = get_model(model_name)()\n",
        "              model[model_name].load_state_dict(torch.load(checkpoint, map_location='cpu'), strict=False)\n",
        "              model[model_name] = checkpoint_wrapper(model[model_name], offload_to_cpu=True)\n",
        "              #model[model_name].load_state_dict(torch.load(\"models/v-diffusion/merged_model.pth\", map_location='cpu'))\n",
        "              model[model_name] = model[model_name].half()\n",
        "              model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
        "          elif model_name == \"openimages\":\n",
        "              openai, diffusion = create_model_and_diffusion(**model_config)\n",
        "              openai.load_state_dict(torch.load(f\"{model_path}/openimages.pth\", map_location='cpu'))\n",
        "              openai.requires_grad_(False).eval().to(device)\n",
        "              for name, param in openai.named_parameters():\n",
        "                  if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "                      param.requires_grad_()\n",
        "              if model_config['use_fp16']:\n",
        "                  openai.convert_to_fp16()\n",
        "              model[\"openimages\"] = wrapped_openai\n",
        "              \n",
        "  if \"cc12m_1_cfg\" in model_list:\n",
        "      model[\"cc12m_1_cfg\"]=cfg_model_fn\n",
        "\n",
        "          \n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                      std=[0.26862954, 0.26130258, 0.27577711])\n",
        "  \n",
        "  for model_name in model_list:\n",
        "    if(model_name != 'wikiart_256'):\n",
        "      pace.append({\"model_name\": model_name, \"guided\": True, \"mag_adjust\": 1})\n",
        "    else:\n",
        "      pace.append({\"model_name\": model_name, \"guided\": True, \"mag_adjust\": 1.5})\n",
        "has_upscaled = False\n",
        "load_diffusion_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VnQjGugaDZPJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Choose your perceptor models\n",
        "\n",
        "# suppress mmc warmup outputs\n",
        "import mmc.loaders\n",
        "clip_load_list = []\n",
        "#@markdown #### Open AI CLIP models\n",
        "ViT_B32 = False #@param {type:\"boolean\"}\n",
        "ViT_B16 = True #@param {type:\"boolean\"}\n",
        "ViT_L14 = False #@param {type:\"boolean\"}\n",
        "ViT_L14_336px = False #@param {type:\"boolean\"}\n",
        "#RN101 = False #@param {type:\"boolean\"}\n",
        "#RN50 = False #@param {type:\"boolean\"}\n",
        "RN50x4 = False #@param {type:\"boolean\"}\n",
        "RN50x16 = False #@param {type:\"boolean\"}\n",
        "RN50x64 = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### OpenCLIP models\n",
        "ViT_B16_plus = False #@param {type: \"boolean\"}\n",
        "ViT_B32_laion2b = True #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown #### Multilangual CLIP models \n",
        "clip_farsi = False #@param {type: \"boolean\"}\n",
        "clip_korean = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown #### CLOOB models\n",
        "cloob_ViT_B16 = False #@param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Load even more CLIP and CLIP-like models (from [Multi-Modal-Comparators](https://github.com/dmarx/Multi-Modal-Comparators))\n",
        "model1 = \"\" # @param [\"[clip - openai - RN50]\",\"[clip - openai - RN101]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "model2 = \"\" # @param [\"[clip - openai - RN50]\",\"[clip - openai - RN101]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "model3 = \"\" # @param [\"[clip - openai - RN50]\",\"[clip - openai - RN101]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "\n",
        "if ViT_B32: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-32--openai]\")\n",
        "if ViT_B16: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-16--openai]\")\n",
        "if ViT_L14: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-L-14--openai]\")\n",
        "if RN50x4: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x4--openai]\")\n",
        "if RN50x64: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x64--openai]\")\n",
        "if RN50x16: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x16--openai]\")\n",
        "if ViT_L14_336px:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-L-14-336--openai]\")\n",
        "if ViT_B16_plus:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-16-plus-240--laion400m_e32]\")\n",
        "if ViT_B32_laion2b:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-32--laion2b_e16]\")\n",
        "if clip_farsi:\n",
        "  clip_load_list.append(\"[clip - sajjjadayobi - clipfa]\")\n",
        "if clip_korean:\n",
        "  clip_load_list.append(\"[clip - navervision - kelip_ViT-B/32]\")\n",
        "if cloob_ViT_B16:\n",
        "  clip_load_list.append(\"[cloob - crowsonkb - cloob_laion_400m_vit_b_16_32_epochs]\")\n",
        "\n",
        "if model1:\n",
        "  clip_load_list.append(model1)\n",
        "if model2:\n",
        "  clip_load_list.append(model2)\n",
        "if model3:\n",
        "  clip_load_list.append(model3)\n",
        "\n",
        "\n",
        "i = 0\n",
        "from mmc.multimmc import MultiMMC\n",
        "from mmc.modalities import TEXT, IMAGE\n",
        "temp_perceptor = MultiMMC(TEXT, IMAGE)\n",
        "\n",
        "def get_mmc_models(clip_load_list):\n",
        "  mmc_models = []\n",
        "  for model_key in clip_load_list:\n",
        "      if not model_key:\n",
        "          continue\n",
        "      arch, pub, m_id = model_key[1:-1].split(' - ')\n",
        "      mmc_models.append({\n",
        "          'architecture':arch,\n",
        "          'publisher':pub,\n",
        "          'id':m_id,\n",
        "          })\n",
        "  return mmc_models\n",
        "mmc_models = get_mmc_models(clip_load_list)\n",
        "\n",
        "import mmc\n",
        "from mmc.registry import REGISTRY\n",
        "import mmc.loaders  # force trigger model registrations\n",
        "from mmc.mock.openai import MockOpenaiClip\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "\n",
        "def load_clip_models(mmc_models):\n",
        "  clip_model, clip_size, clip_tokenize, clip_normalize= {},{},{},{}\n",
        "  clip_list = []\n",
        "  for item in mmc_models:\n",
        "      print(\"Loaded \", item[\"id\"])\n",
        "      clip_list.append(item[\"id\"])\n",
        "      model_loaders = REGISTRY.find(**item)\n",
        "      for model_loader in model_loaders:\n",
        "          clip_model_loaded = model_loader.load()\n",
        "          clip_model[item[\"id\"]] = MockOpenaiClip(clip_model_loaded)\n",
        "          clip_size[item[\"id\"]] = clip_model[item[\"id\"]].visual.input_resolution\n",
        "          clip_tokenize[item[\"id\"]] = clip_model[item[\"id\"]].preprocess_text()\n",
        "          if(item[\"architecture\"] == 'cloob'):\n",
        "            clip_normalize[item[\"id\"]] = clip_model[item[\"id\"]].normalize\n",
        "          else:\n",
        "            clip_normalize[item[\"id\"]] = normalize\n",
        "  return clip_model, clip_size, clip_tokenize, clip_normalize, clip_list\n",
        "\n",
        "\n",
        "def full_clip_load(clip_load_list):\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  try:\n",
        "    del clip_model, clip_size, clip_tokenize, clip_normalize, clip_list\n",
        "  except:\n",
        "    pass\n",
        "  mmc_models = get_mmc_models(clip_load_list)\n",
        "  clip_model, clip_size, clip_tokenize, clip_normalize, clip_list = load_clip_models(mmc_models)\n",
        "  return clip_model, clip_size, clip_tokenize, clip_normalize, clip_list\n",
        "\n",
        "clip_model, clip_size, clip_tokenize, clip_normalize, clip_list = full_clip_load(clip_load_list)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQN0NTcCUtS"
      },
      "source": [
        "## More setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U0PwzFZbLfcy"
      },
      "outputs": [],
      "source": [
        "# @title Setup cond_model and cond_sample\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import threading\n",
        "\n",
        "from tqdm.auto import trange\n",
        "\n",
        "def make_cond_model_fn(model, cond_fn):\n",
        "    def cond_model_fn(x, t, **extra_args):\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                if lerp:\n",
        "                    v=torch.zeros_like(x)\n",
        "                    for j in pace:\n",
        "                        if j[\"model_name\"]==\"cc12m_1_cfg\" or j[\"model_name\"]==\"cc12m_1\" or j[\"model_name\"]==\"nshep_danbooru\":\n",
        "                            extra_args_in = extra_args\n",
        "                        else:\n",
        "                            extra_args_in= {}\n",
        "                        v += model[j[\"model_name\"]](x, t, **extra_args_in)\n",
        "\n",
        "                    v = v/len(pace)\n",
        "                else:\n",
        "                    v = model[pace[i%len(pace)][\"model_name\"]](x, t, **extra_args_in)\n",
        "                alphas, sigmas = utils.t_to_alpha_sigma(t)\n",
        "                pred = x * alphas[:, None, None, None] - v * sigmas[:, None, None, None]\n",
        "                cond_grad = cond_fn(x, t, pred, **extra_args).detach()\n",
        "                v = v.detach() - cond_grad * (sigmas[:, None, None, None] / alphas[:, None, None, None])\n",
        "        return v\n",
        "    return cond_model_fn\n",
        "\n",
        "def cond_clamp(image): \n",
        "    #if t >=0:\n",
        "        mag=image.square().mean().sqrt()\n",
        "        mag = (mag*cc).clamp(1.6,100)\n",
        "        image = image.clamp(-mag, mag)\n",
        "        return(image)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cond_sample(model, x, steps, eta_schedule, extra_args, cond_fn):\n",
        "    \"\"\"Draws guided samples from a model given starting noise.\"\"\"\n",
        "    global clamp_max\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "    # Create the noise schedule\n",
        "    alphas, sigmas = utils.t_to_alpha_sigma(steps)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(len(steps)):\n",
        "        #if stop_flag: break\n",
        "        if pace[i%len(pace)][\"model_name\"]==\"cc12m_1_cfg\" or pace[i%len(pace)][\"model_name\"]==\"cc12m_1\" or pace[i%len(pace)][\"model_name\"]==\"nshep_danbooru\":\n",
        "            extra_args_in = extra_args\n",
        "        else:\n",
        "            extra_args_in= {}\n",
        "\n",
        "        # Get the model output\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                if lerp:\n",
        "                    v=torch.zeros_like(x)\n",
        "                    for j in pace:\n",
        "                        if j[\"model_name\"]==\"cc12m_1_cfg\" or j[\"model_name\"]==\"cc12m_1\" or j[\"model_name\"]==\"nshep_danbooru\":\n",
        "                            extra_args_in = extra_args\n",
        "                        else:\n",
        "                            extra_args_in= {}\n",
        "                        v += model[j[\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
        "                        \n",
        "                    v = v/len(pace)\n",
        "                else:\n",
        "                    v = model[pace[i%len(pace)][\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
        "            v = cond_clamp(v)\n",
        "        if torch.isnan(v).any(): continue\n",
        "        \n",
        "        if use_secondary_model:\n",
        "            with torch.no_grad():\n",
        "                if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
        "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
        "                else:\n",
        "                    v = v.detach()\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    clamp_max=torch.tensor([0])\n",
        "\n",
        "        else:\n",
        "            if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
        "                with torch.enable_grad():\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
        "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    v = v.detach()\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    clamp_max=torch.tensor([0])\n",
        "\n",
        "        mag = pred.square().mean().sqrt()\n",
        "        #print(mag)\n",
        "        if torch.isnan(mag):\n",
        "            print(\"ERROR2\")\n",
        "            continue\n",
        "            \n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < len(steps) - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            if eta_schedule[i] >=0:\n",
        "                ddim_sigma = eta_schedule[i] * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                    (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            else:\n",
        "                ddim_sigma = -eta_schedule[i]*sigmas[i+1]\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "            x = cond_clamp(x)\n",
        "\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta_schedule[i]:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "            \n",
        "         #######   x = sample_a_step(model, x.detach(), steps2, i//2, eta, extra_args)\n",
        "\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oSl6fprQBKTT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title Setup cond_fn \n",
        "clamp_start_=0\n",
        "\n",
        "def centralized_grad(x, use_gc=True, gc_conv_only=False):\n",
        "    if use_gc:\n",
        "        if gc_conv_only:\n",
        "            if len(list(x.size())) > 3:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "        else:\n",
        "            if len(list(x.size())) > 1:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "    return x\n",
        "\n",
        "def cond_fn(x, t, x_in, clip_embed=[]):\n",
        "    t2 = t\n",
        "    t=1000-t*1000\n",
        "    t=round(t[0].item())\n",
        "    with torch.enable_grad():\n",
        "        global test, clamp_start_, clamp_max\n",
        "        n = x.shape[0]\n",
        "        if use_secondary_model:                 \n",
        "            x = x.detach().requires_grad_()\n",
        "            x_in_second = secondary_model(x, t2.repeat([n])).pred\n",
        "            if use_original_as_clip_in: x_in = replace_grad(x_in, (1-use_original_as_clip_in)*x_in_second+use_original_as_clip_in*x_in)\n",
        "            else : x_in = x_in_second\n",
        "        display_handling(x_in,t)\n",
        "        n = x_in.shape[0]\n",
        "        clip_guidance_scale = clip_guidance_index[t]\n",
        "        make_cutouts = {}\n",
        "        x_in_grad = torch.zeros_like(x_in)\n",
        "        for i in clip_list:\n",
        "            make_cutouts[i] = MakeCutouts(clip_size[i],\n",
        "             Overview= cut_overview[t], \n",
        "             InnerCrop = cut_innercut[t], \n",
        "             IC_Size_Pow=cut_ic_pow, \n",
        "             IC_Grey_P = cut_icgray_p[t]\n",
        "             )\n",
        "            cutn = cut_overview[t]+cut_innercut[t]\n",
        "        for j in range(cutn_batches):\n",
        "            losses=0\n",
        "            for i in clip_list:\n",
        "                clip_in = clip_normalize[i](make_cutouts[i](x_in.add(1).div(2)).to(\"cuda\"))\n",
        "                image_embeds = clip_model[i].encode_image(clip_in).float().unsqueeze(0).expand([target_embeds[i].shape[0],-1,-1])\n",
        "                target_embeds_temp = target_embeds[i]\n",
        "                if i == 'ViT-B-32--openai' and experimental_aesthetic_embeddings:\n",
        "                  aesthetic_embedding = torch.from_numpy(np.load(f'aesthetic-predictor/vit_b_32_embeddings/rating{experimental_aesthetic_embeddings_score}.npy')).to(device) \n",
        "                  aesthetic_query = target_embeds_temp + aesthetic_embedding * experimental_aesthetic_embeddings_weight\n",
        "                  target_embeds_temp = (aesthetic_query) / torch.linalg.norm(aesthetic_query)\n",
        "                if i == 'ViT-L-14--openai' and experimental_aesthetic_embeddings:\n",
        "                  aesthetic_embedding = torch.from_numpy(np.load(f'aesthetic-predictor/vit_l_14_embeddings/rating{experimental_aesthetic_embeddings_score}.npy')).to(device) \n",
        "                  aesthetic_query = target_embeds_temp + aesthetic_embedding * experimental_aesthetic_embeddings_weight\n",
        "                  target_embeds_temp = (aesthetic_query) / torch.linalg.norm(aesthetic_query)  \n",
        "                target_embeds_temp = target_embeds_temp.unsqueeze(1).expand([-1,cutn*n,-1])\n",
        "                dists = spherical_dist_loss(image_embeds, target_embeds_temp)\n",
        "                dists = dists.mean(1).mul(weights[i].squeeze()).mean()\n",
        "                losses+=dists*clip_guidance_scale * (2 if i in [\"ViT-L-14-336--openai\", \"RN50x64--openai\", \"ViT-B-32--laion2b_e16\"] else (.4 if \"cloob\" in i else 1))\n",
        "                if i == \"ViT-L-14-336--openai\" and aes_scale !=0:\n",
        "                    aes_loss = (aesthetic_model_336(F.normalize(image_embeds, dim=-1))).mean() \n",
        "                    losses -= aes_loss * aes_scale \n",
        "                if i == \"ViT-L-14--openai\" and aes_scale !=0:\n",
        "                    aes_loss = (aesthetic_model_224(F.normalize(image_embeds, dim=-1))).mean() \n",
        "                    losses -= aes_loss * aes_scale \n",
        "                if i == \"ViT-B-16--openai\" and aes_scale !=0:\n",
        "                    aes_loss = (aesthetic_model_16(F.normalize(image_embeds, dim=-1))).mean() \n",
        "                    losses -= aes_loss * aes_scale \n",
        "                if i == \"ViT-B-32--openai\" and aes_scale !=0:\n",
        "                    aes_loss = (aesthetic_model_32(F.normalize(image_embeds, dim=-1))).mean()\n",
        "                    losses -= aes_loss * aes_scale\n",
        "                #losses += dists\n",
        "                #losses = losses / len(clip_list)                \n",
        "                #gc.collect()\n",
        " \n",
        "        tv_losses = tv_loss(x_in).sum() * tv_scales[0] +\\\n",
        "            tv_loss(F.interpolate(x_in, scale_factor= 1/2)).sum()* tv_scales[1] + \\\n",
        "            tv_loss(F.interpolate(x_in, scale_factor = 1/4)).sum()* tv_scales[2] + \\\n",
        "            tv_loss(F.interpolate(x_in, scale_factor = 1/8)).sum()* tv_scales[3] \n",
        "        range_scale= range_index[t]\n",
        "        range_losses = range_loss(x_in,RGB_min,RGB_max).sum() * range_scale\n",
        "        loss =  tv_losses  + range_losses + losses\n",
        "        if symmetric_loss_scale != 0: loss +=  symmetric_loss(x_in) * symmetric_loss_scale\n",
        "        if init_image is not None and init_scale:\n",
        "            lpips_loss = (lpips_model(x_in, init) * init_scale).squeeze().mean()\n",
        "            #print(lpips_loss)\n",
        "            loss += lpips_loss\n",
        "        loss.backward()\n",
        "        grad = -x.grad\n",
        "        grad = torch.nan_to_num(grad, nan=0.0, posinf=0, neginf=0)\n",
        "        if grad_center: grad = centralized_grad(grad, use_gc=True, gc_conv_only=False)\n",
        "        mag = grad.square().mean().sqrt()\n",
        "        if mag==0 or torch.isnan(mag):\n",
        "            print(\"ERROR\")\n",
        "            print(t)\n",
        "            return(grad)\n",
        "        if t>=0:\n",
        "            if active_function == \"softsign\":\n",
        "                grad = F.softsign(grad*grad_scale/mag)\n",
        "            if active_function == \"tanh\":\n",
        "                grad = (grad/mag*grad_scale).tanh()\n",
        "            if active_function==\"clamp\":\n",
        "                grad = grad.clamp(-mag*grad_scale*2,mag*grad_scale*2)\n",
        "        if grad.abs().max()>0:\n",
        "            grad=grad/grad.abs().max()*mag_mul\n",
        "            magnitude = grad.square().mean().sqrt()\n",
        "        else:\n",
        "            return(grad)\n",
        "        clamp_max = clamp_index[t]\n",
        "        #print(magnitude, end = \"\\r\")\n",
        "        grad = grad* magnitude.clamp(max= clamp_max) /magnitude#0.2\n",
        "        grad = grad.detach()\n",
        "    return grad\n",
        "\n",
        "def null_fn(x_in):\n",
        "    return(torch.zeros_like(x_in))\n",
        "\n",
        "def display_handling(x_in,t):\n",
        "    global progress\n",
        "    filename = f'{outputs_path}/{taskname}_N.jpg'\n",
        "    if torch.isnan(x_in).any(): return()\n",
        "    TF.to_pil_image(x_in[0].add(1).div(2).clamp(0, 1)).save(filename,quality=99)\n",
        "    settings = generate_settings_file(add_prompts=True, add_dimensions=True)\n",
        "    text_file = open(f\"{outputs_path}/{taskname}_N.cfg\", \"w\")\n",
        "    text_file.write(settings)\n",
        "    text_file.close()\n",
        "    textprogress.value = f'{taskname},  step {round(t*1000)}'\n",
        "    file = open(filename, \"rb\")\n",
        "    image=file.read()\n",
        "    progress.value = image \n",
        "    file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VzW4ffEOBKTU"
      },
      "outputs": [],
      "source": [
        "# @title Load aesthetic model\n",
        "aesthetic_model_336 = torch.nn.Linear(768,1).cuda()\n",
        "aesthetic_model_336.load_state_dict(torch.load(f\"{model_path}/ava_vit_l_14_336_linear.pth\"))\n",
        "\n",
        "aesthetic_model_224 = torch.nn.Linear(768,1).cuda()\n",
        "aesthetic_model_224.load_state_dict(torch.load(f\"{model_path}/ava_vit_l_14_linear.pth\"))\n",
        "\n",
        "aesthetic_model_16 = torch.nn.Linear(512,1).cuda()\n",
        "aesthetic_model_16.load_state_dict(torch.load(f\"{model_path}/ava_vit_b_16_linear.pth\"))\n",
        "\n",
        "aesthetic_model_32 = torch.nn.Linear(512,1).cuda()\n",
        "aesthetic_model_32.load_state_dict(torch.load(f\"{model_path}/sa_0_4_vit_b_32_linear.pth\"))\n",
        "\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EI-Dt-2KWRb6"
      },
      "outputs": [],
      "source": [
        "# @title Main functions\n",
        "\n",
        "#Make ETA schedule proportional to number of steps\n",
        "def eta_schedule_proportional(eta_index):\n",
        "  list_mul_eta = list_mul_to_array(eta_index)\n",
        "  import re\n",
        "  multipliers = re.findall(\"\\*(\\d+)\", list_mul_eta)\n",
        "  multiplied = re.findall(\"\\[(\\d+\\.\\d+)]\", list_mul_eta)\n",
        "  int_multipliers = [int(numeric_string) for numeric_string in multipliers]\n",
        "  sum_totals = sum(int_multipliers)\n",
        "  if(sum_totals != step):\n",
        "    proportion = step/sum_totals\n",
        "    new_multiplication_string = ''\n",
        "    i = 0\n",
        "    for multiplier in int_multipliers:\n",
        "      new_multiplier = math.ceil(multiplier*proportion)\n",
        "      new_multiplication_string += f' [{multiplied[i]}]*{new_multiplier} +'\n",
        "      i+=1\n",
        "  else:\n",
        "    return(eta_index)  \n",
        "  return(eval(new_multiplication_string[1:-2]))\n",
        "\n",
        "#Convert a giant array into a string to be used in settigns\n",
        "def list_mul_to_array(list_mul):\n",
        "  i = 0\n",
        "  mul_count = 0\n",
        "  mul_string = ''\n",
        "  full_list = list_mul\n",
        "  full_list_len = len(full_list)\n",
        "  for item in full_list:\n",
        "    if(i == 0):\n",
        "      last_item = item\n",
        "    if(item == last_item):\n",
        "      mul_count+=1\n",
        "    if(item != last_item or full_list_len == i+1):\n",
        "      mul_string = mul_string + f' [{last_item}]*{mul_count} +'\n",
        "      mul_count=1\n",
        "    last_item = item\n",
        "    i+=1\n",
        "  clean_string = mul_string[1:-2]\n",
        "  if(not clean_string):\n",
        "    clean_string = \"[]\"\n",
        "  return(clean_string)\n",
        "\n",
        "def generate_settings_file(add_prompts=False, add_dimensions=False):\n",
        "  \n",
        "  if(add_prompts):\n",
        "    prompts_list = f'''\n",
        "    prompts = {prompts}\n",
        "    image_prompts = {image_prompts}\n",
        "    '''\n",
        "  else:\n",
        "    prompts_list = ''\n",
        "\n",
        "  if(add_dimensions):\n",
        "    dimensions = f'''width = {width}\n",
        "  \theight = {height}\n",
        "    '''\n",
        "  else:\n",
        "    dimensions = ''\n",
        "  settings = f'''\n",
        "    #This settings file can be loaded back to V-Majesty Diffusion.\n",
        "    #If you like your setting consider sharing it to the settings library at https://github.com/multimodalart/MajestyDiffusion\n",
        "    [model_list]\n",
        "    model_list = {model_list}\n",
        "    \n",
        "    [clip_list]\n",
        "    perceptors = {clip_load_list}\n",
        "    \n",
        "    [basic_settings]\n",
        "    #Perceptor things\n",
        "    {prompts_list}\n",
        "    {dimensions}\n",
        "    clip_guidance_scale = {clip_guidance_scale}\n",
        "    step = {step}\n",
        "    aesthetic_loss_scale = {aesthetic_loss_scale}\n",
        "    augment_cuts={augment_cuts}\n",
        "\n",
        "    #Init image settings\n",
        "    starting_timestep = {starting_timestep}\n",
        "    init_scale = {init_scale} \n",
        "    mask_scale = {mask_scale}\n",
        "\n",
        "    [advanced_settings]\n",
        "    #Add CLIP Guidance and all the flavors or just run normal Latent Diffusion\n",
        "    \n",
        "    use_secondary_model={use_secondary_model}\n",
        "    use_original_as_clip_in={use_original_as_clip_in}\n",
        "    lerp={lerp}\n",
        "    #Cut settings\n",
        "    cut_overview = {list_mul_to_array(cut_overview)}\n",
        "    cut_innercut = {list_mul_to_array(cut_innercut)}\n",
        "    cut_ic_pow = {cut_ic_pow}\n",
        "    cut_icgray_p = {list_mul_to_array(cut_icgray_p)}\n",
        "    cutn_batches = {cutn_batches}\n",
        "    range_index = {list_mul_to_array(range_index)}\n",
        "    eta_index = {list_mul_to_array(eta_index)}\n",
        "    active_function = \"{active_function}\"\n",
        "    tv_scales = {list_mul_to_array(tv_scales)}\n",
        "    n_batches = {n_batches}\n",
        "    step_enhance={step_enhance}\n",
        "    mid_point = {mid_point}\n",
        "    steps_pow = {steps_pow}\n",
        "    #cfg_scale only for cc12m_cfg\n",
        "    cfg_scale = {cfg_scale}\n",
        "    #If you uncomment this line you can schedule the CLIP guidance across the steps. Otherwise the clip_guidance_scale will be used\n",
        "    clip_guidance_schedule = {list_mul_to_array(clip_guidance_index)}\n",
        "    \n",
        "    #Apply symmetric loss (force simmetry to your results)\n",
        "    symmetric_loss_scale = {symmetric_loss_scale} \n",
        "\n",
        "    #Grad and mag advanced settings\n",
        "    grad_center = {grad_center}\n",
        "    #Lower value result in more coherent and detailed result, higher value makes it focus on more dominent concept\n",
        "    grad_scale={grad_scale} \n",
        "    mag_mul = {mag_mul}\n",
        "    clamp_start_={clamp_start_}\n",
        "    clamp_index = {list_mul_to_array(clamp_index)}\n",
        "    \n",
        "    #More settings\n",
        "    RGB_min = {RGB_min}\n",
        "    RGB_max = {RGB_max}\n",
        "    #How to pad the image with cut_overview\n",
        "    padargs = {padargs} \n",
        "    flip_aug={flip_aug}\n",
        "    cc = {cc}\n",
        "    #Experimental aesthetic embeddings, work only with OpenAI ViT-B/32 and ViT-L/14\n",
        "    experimental_aesthetic_embeddings = {experimental_aesthetic_embeddings}\n",
        "    #How much you want this to influence your result\n",
        "    experimental_aesthetic_embeddings_weight = {experimental_aesthetic_embeddings_weight}\n",
        "    #9 are good aesthetic embeddings, 0 are bad ones\n",
        "    experimental_aesthetic_embeddings_score = {experimental_aesthetic_embeddings_score}\n",
        "\n",
        "    #Internal upscaler settings\n",
        "    activate_upscaler = {activate_upscaler}\n",
        "    upscale_model = \"{upscale_model}\"\n",
        "    multiply_image_size_by = {multiply_image_size_by}\n",
        "    '''\n",
        "  return(settings)\n",
        "def do_run():\n",
        "    global target_embeds, weights, init, makecutouts, progress, textprogress, progress2, batch_num, taskname\n",
        "    with torch.cuda.amp.autocast():\n",
        "        if seed is not None:\n",
        "            torch.manual_seed(seed)\n",
        "        make_cutouts = {}\n",
        "        for i in clip_list:\n",
        "             make_cutouts[i] = MakeCutouts(clip_size[i],Overview=1)\n",
        "        side_x, side_y = [w,h]\n",
        "        target_embeds, weights ,zero_embed = {}, {}, {}\n",
        "        for i in clip_list:\n",
        "            target_embeds[i] = []\n",
        "            weights[i]=[]\n",
        "\n",
        "        \n",
        "            \n",
        "        for prompt in prompts:\n",
        "            txt, weight = parse_prompt(prompt)\n",
        "            for i in clip_list:\n",
        "                embeds = clip_model[i].encode_text(clip.tokenize(txt).to(device)).float()\n",
        "                target_embeds[i].append(embeds)\n",
        "                weights[i].append(weight)\n",
        "        for prompt in image_prompts:\n",
        "            print(f\"processing{prompt}\",end=\"\\r\")\n",
        "            path, weight = parse_prompt(prompt)\n",
        "            img = Image.open(fetch(path)).convert('RGB')\n",
        "            img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "            for i in clip_list:\n",
        "                batch = make_cutouts[i](TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "                embed = clip_model[i].encode_image(normalize(batch)).float()\n",
        "                target_embeds[i].append(embed)\n",
        "                weights[i].extend([weight])\n",
        "\n",
        "        #if anti_jpg!=0:\n",
        "        #    if \"ViT-B/32\" not in clip_list:\n",
        "        #      target_embeds[\"ViT-B/32\"] = []\n",
        "        #      weights[\"ViT-B/32\"] = []\n",
        "        #    target_embeds[\"ViT-B/32\"].append(torch.tensor([np.load(f\"{model_path}/openimages_512x_png_embed224.npz\")['arr_0']-np.load(f\"{model_path}/imagenet_512x_jpg_embed224.npz\")['arr_0']], device = device))\n",
        "        #    weights[\"ViT-B/32\"].append(anti_jpg)\n",
        "\n",
        "        #print(weights)\n",
        "        for i in clip_list:\n",
        "            target_embeds[i] = torch.cat(target_embeds[i])\n",
        "            weights[i] = torch.tensor([*weights[i]], device=device)\n",
        "\n",
        "        init = None\n",
        "        init_mask = None\n",
        "        if init_image is not None:\n",
        "            S = model_config['image_size']\n",
        "            if mask_scale > 0:\n",
        "                init = Image.open(fetch(init_image)).convert('RGBA')\n",
        "                init = init.resize((S, S), Image.BILINEAR)\n",
        "                init = TF.to_tensor(init).to(device)\n",
        "                init_mask = init[3] # alpha channel\n",
        "                init_mask = (init_mask>0.5).to(torch.float32)\n",
        "                init = init[:3].unsqueeze(0).mul(2).sub(1) # RGB\n",
        "            else:\n",
        "                init = Image.open(fetch(init_image)).convert('RGB')\n",
        "                init = init.resize((S, S), Image.LANCZOS)\n",
        "                init = TF.to_tensor(init).to(device)\n",
        "                init = init.unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "        cur_t = None\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            taskname=taskname_+\"_\"+str(i)\n",
        "            from IPython.display import display\n",
        "            import ipywidgets as widgets\n",
        "            import threading\n",
        "\n",
        "            t = torch.linspace(1, 0, step + 1, device=device)[:-1]\n",
        "            if step_enhance:\n",
        "                t = torch.tensor(np.concatenate([np.arange(1,mid_point,(mid_point-1)/step/0.5),np.arange(mid_point,0,-mid_point/step/0.5)])).to(\"cuda\")\n",
        "            x = torch.randn([1, 3, side_y, side_x], device=device)\n",
        "            steps = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "            if init_image is not None:\n",
        "                steps = steps[steps < starting_timestep]\n",
        "                alpha, sigma = utils.t_to_alpha_sigma(steps[0])\n",
        "                x = init * alpha + x * sigma\n",
        "            if \"cc12m_1_cfg\" in model_list or \"cc12m_1\" in model_list or \"nshep_danbooru\" in model_list:\n",
        "                extra_args = {'clip_embed': target_embeds[\"ViT-B-16--openai\"][0].unsqueeze(0)}\n",
        "            else:\n",
        "                extra_args = {}\n",
        "            progress = widgets.Image(layout = widgets.Layout(max_width = \"400px\",max_height = \"512px\"))\n",
        "            textprogress = widgets.Textarea()\n",
        "            display(textprogress)\n",
        "            display(progress)\n",
        "            if sampling_method == \"DDIM\":\n",
        "                cond_sample(model, x, steps, eta_index, extra_args, cond_fn)\n",
        "            if sampling_method == \"PLMS\":\n",
        "                model_fn = make_cond_model_fn(model, cond_fn)\n",
        "                sampling.plms_sample(model_fn, x, steps, extra_args, callback=None)\n",
        "            if sampling_method == \"PLMS2\":\n",
        "                model_fn = make_cond_model_fn(model, cond_fn)\n",
        "                sampling.plms2_sample(model_fn, x, steps, extra_args, callback=None)\n",
        "            if sampling_method == \"PIE\":\n",
        "                model_fn = make_cond_model_fn(model, cond_fn)\n",
        "                sampling.pie_sample(model_fn, x, steps, extra_args, callback=None)\n",
        "            if sampling_method == \"PRK\":\n",
        "                model_fn = make_cond_model_fn(model, cond_fn)\n",
        "                sampling.prk_sample(model_fn, x, steps, extra_args, callback=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KLrc3vrCcox"
      },
      "source": [
        "## Diffuse!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6coIeiGWWca"
      },
      "source": [
        "### Advanced settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ne0GgCWWBy"
      },
      "outputs": [],
      "source": [
        "RGB_min, RGB_max = [-0.95,0.95]\n",
        "n_batches = 1\n",
        "#cutn_batches seem to be ignored at the moment as gradient caching is being used, so increase your actual cuts\n",
        "cutn_batches = 1\n",
        "#unified_cutouts = True #deprecated\n",
        "#ns_cutn = 10 #deprecated\n",
        "cut_overview = [24]*1000\n",
        "cut_innercut = [0]*200+[0]* 1000\n",
        "cut_ic_pow = 0.5\n",
        "cut_icgray_p = [0.2]*100+[0]*100+[0]*100+[0]*1000   \n",
        "\n",
        "#If you uncomment next line you can schedule the CLIP guidance across the steps. Otherwise the clip_guidance_scale basic setting will be used\n",
        "#clip_guidance_schedule = [5000]*300 + [1000]*700\n",
        "\n",
        "padargs = {\"mode\":\"constant\", \"value\":-1}\n",
        "flip_aug=False\n",
        "cutout_debug = False\n",
        "\n",
        "tv_scales = [150]*4\n",
        "#tv_scale_2 = [150]*0 #deprecated\n",
        "\n",
        "#Shifts the balance between the steps, if midpoint is > 0.5, it will privilege later steps, < 0.5 will privilege early ones\n",
        "step_enhance=True\n",
        "mid_point = 0.6\n",
        "#Lower than 1 means giving more power to earlier cuts, higher than 1 gives more power to higher cuts\n",
        "steps_pow= 1\n",
        "cfg_scale = 3\n",
        "\n",
        "clamp_index = 1 * np.array([0.03]*50+[0.04]*200+[0.05]*750)\n",
        "#sat_index =   0 * np.array([10000]*40+[0]*1000) #deprecated\n",
        "range_index =  [1500000]*100+[0]*1000\n",
        "eta_index = [1.2]*100\n",
        "symmetric_loss_scale = 0\n",
        "grad_center = False\n",
        "mag_mul = 1\n",
        "clamp_start_=0\n",
        "\n",
        "#0-0.999... when secondary model is on - merges the secondary and first model grads\n",
        "use_original_as_clip_in=0\n",
        "lerp=True\n",
        "sampling_method=\"DDIM\" #PLMS is broken right now\n",
        "\n",
        "#perlin_init=False #deprecated\n",
        "#anti_jpg=0.5 #broken\n",
        "\n",
        "#Experimental aesthetic embeddings, work only with OpenAI ViT-B/32 and ViT-L/14\n",
        "experimental_aesthetic_embeddings = False\n",
        "#How much you want this to influence your result\n",
        "experimental_aesthetic_embeddings_weight = 0.5\n",
        "#9 are good aesthetic embeddings, 0 are bad ones\n",
        "experimental_aesthetic_embeddings_score = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftFRiUO1WZYR"
      },
      "source": [
        "### Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ahcWURYWnL"
      },
      "outputs": [],
      "source": [
        "# Prompts\n",
        "#Amp up your prompt game with prompt engineering, check out this guide: https://matthewmcateer.me/blog/clip-prompt-engineering/\n",
        "prompts = [\"A Majestic Castle by Studio Ghibli\"]\n",
        "\n",
        "# Image prompts\n",
        "image_prompts = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "X5gODNAMEUCR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import random\n",
        "#import threading\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "#@markdown ### Basic settings \n",
        "#@markdown We're still figuring out default settings. Experiment and <a href=\"https://github.com/multimodalart/majesty-diffusion\">share your settings with us</a>\n",
        "#@markdown Experiment with lower `width` and `height` that is then further upscaled with yfcc and openclip, works great \n",
        "width =  512#@param{type: 'integer'}\n",
        "height =  512#@param{type: 'integer'}\n",
        "clip_guidance_scale =  2400#@param{type: 'integer'}\n",
        "step =  100#@param{type: 'integer'}\n",
        "aesthetic_loss_scale = 100 #@param{type: 'integer'}\n",
        "augment_cuts=True #@param{type:'boolean'}\n",
        "use_secondary_model=False#@param{type:'boolean'}\n",
        "stop_flag = False\n",
        "batch_num=0\n",
        "seed = int(random.randint(0, 2147483647))\n",
        "batch_title = \"creations\"\n",
        "title = batch_title\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown  ### Init image settings\n",
        "#@markdown `init_image` requires the path of an image to use as init to the model\n",
        "init_image = None #@param{type: 'string'}\n",
        "if(init_image == '' or init_image == 'None'):\n",
        "  init_image = None\n",
        "#@markdown `init_mask` is a mask same width and height as the original image with the color black indicating where to inpaint\n",
        "init_mask = None #@param{type: 'string'}\n",
        "mask_scale=0\n",
        "#@markdown `init_scale` controls how much the init image should influence the final result. Experiment with values around `1000`\n",
        "init_scale = 1000 #@param{type: 'integer'}\n",
        "#@markdown If you are used to `skip_timesteps` for init images, this is it but as a % of noise you would like to add\n",
        "starting_timestep =  0.9#@param{type: 'number'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#Get corrected sizes\n",
        "w = (width//64)*64;\n",
        "h = (height//64)*64;\n",
        "if w != width or h != height:\n",
        "    print(f'Changing output size to {w}x{h}. Dimensions must by multiples of 64.')\n",
        "#w,h = width,height\n",
        "#@markdown  ### Internal Upscale (upscale the output with a bigger model)\n",
        "activate_upscaler = False #@param{type: 'boolean'}\n",
        "upscale_model = 'yfcc_2' #@param [\"yfcc_2\", \"imagenet_openimages\"]\n",
        "if(upscale_model == 'imagenet_openimages'):\n",
        "  upscale_model = 'openimages'\n",
        "upscale_steps = 100 #@param{type: 'integer'}\n",
        "upscale_starting_timestep = 0.8 #@param{type: 'number'}\n",
        "multiply_image_size_by = 2 #@param{type: 'integer'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### Custom saved settings\n",
        "#@markdown If you choose custom saved settings, the settings set by the preset overrule some of your choices. You can still modify the settings not in the preset. <a href=\"https://github.com/multimodalart/majesty-diffusion/tree/main/v_settings_library\">Check what each preset modifies here</a>\n",
        "custom_settings = 'path/to/settings.cfg' #@param{type:'string'}\n",
        "settings_library = 'None (use settings defined above)' #@param [\"None (use settings defined above)\", \"default (optimized for colab free)\", \"disco_diffusion_defaults\"]\n",
        "if(settings_library != 'None (use settings defined above)'):\n",
        "  if(settings_library == 'default (optimized for colab free)'):\n",
        "    custom_settings = f'majesty-diffusion/v_settings_library/default.cfg'\n",
        "  else:\n",
        "    custom_settings = f'majesty-diffusion/v_settings_library/{settings_library}.cfg'\n",
        "\n",
        "is_custom_settings = (custom_settings is not None and custom_settings is not '' and custom_settings != 'path/to/settings.cfg')\n",
        "\n",
        "#Reload the user selected models after an upscale or after they remove a settings file\n",
        "if(has_upscaled or (has_loaded_custom and not is_custom_settings)):\n",
        "  del model\n",
        "  load_diffusion_models(reload=True)\n",
        "\n",
        "global_var_scope = globals()\n",
        "has_loaded_custom = False\n",
        "if(is_custom_settings):\n",
        "  has_loaded_custom = True\n",
        "  print('Loaded ', custom_settings)\n",
        "  try:\n",
        "    from configparser import ConfigParser\n",
        "  except ImportError:\n",
        "      from ConfigParser import ConfigParser\n",
        "  import configparser\n",
        "  \n",
        "  config = ConfigParser()\n",
        "  config.read(custom_settings)\n",
        "\n",
        "  #Load diffusion models from config\n",
        "  if(config.has_section('model_list')):\n",
        "    models_incoming_list = config.items('model_list')\n",
        "    incoming_models = models_incoming_list[0]\n",
        "    incoming_models = eval(incoming_models[1])\n",
        "    if((len(incoming_models) != len(model_list)) or not all(elem in incoming_models for elem in model_list)):\n",
        "      pace = []\n",
        "      model_list = incoming_models\n",
        "      load_diffusion_models(reload=False)\n",
        "  #Load CLIP models from config\n",
        "  if(config.has_section('clip_list')):\n",
        "    clip_incoming_list = config.items('clip_list')\n",
        "    clip_incoming_models = clip_incoming_list[0]\n",
        "    incoming_perceptors = eval(clip_incoming_models[1])\n",
        "    if((len(incoming_perceptors) != len(clip_load_list)) or not all(elem in incoming_perceptors for elem in clip_load_list)):\n",
        "      clip_load_list = incoming_perceptors\n",
        "      clip_model, clip_size, clip_tokenize, clip_normalize, clip_list = full_clip_load(clip_load_list)\n",
        "\n",
        "  #Load settings from config and replace variables\n",
        "  if(config.has_section('basic_settings')):\n",
        "    basic_settings = config.items('basic_settings')\n",
        "    for basic_setting in basic_settings:\n",
        "      global_var_scope[basic_setting[0]] = eval(basic_setting[1])\n",
        "  \n",
        "  if(config.has_section('advanced_settings')):\n",
        "    advanced_settings = config.items('advanced_settings')\n",
        "    for advanced_setting in advanced_settings:\n",
        "      global_var_scope[advanced_setting[0]] = eval(advanced_setting[1])\n",
        "\n",
        "aes_scale = aesthetic_loss_scale\n",
        "aug=augment_cuts\n",
        "eta_index=eta_schedule_proportional(eta_index)\n",
        "\n",
        "try: \n",
        "  clip_guidance_schedule\n",
        "  clip_guidance_index = clip_guidance_schedule\n",
        "except:\n",
        "  clip_guidance_index = [clip_guidance_scale]*1000\n",
        "\n",
        "for cc in [6]:\n",
        "        for bsq_scale in [.1]:\n",
        "              for grad_scale in [.1]:\n",
        "                 for active_function in [\"softsign\"]:\n",
        "                    torch.manual_seed(seed)\n",
        "                    random.seed(seed)\n",
        "                    if grad_scale!=1 and active_function==\"NA\": continue\n",
        "                    title2 = title + str(int(time.time()))\n",
        "                    taskname_ = title2 +\"_cc\"+str(cc)+\"_gs\"+str(grad_scale)#+ prompts[0]\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    do_run()\n",
        "                    #threading.Thread(target=do_run, args=()).start()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "has_upscaled = False\n",
        "if(activate_upscaler):\n",
        "  has_upscaled = True\n",
        "  already_loaded = upscale_model in model_list\n",
        "  model_list = []\n",
        "  pace = []    \n",
        "  if(upscale_model == 'yfcc_2'):\n",
        "    model_list.append('yfcc_2')\n",
        "  elif(upscale_model == 'openimages'):\n",
        "    model_list.append('openimages')\n",
        "  \n",
        "  if(not already_loaded):\n",
        "    del model\n",
        "    model = {}\n",
        "    load_diffusion_models(reload=False)\n",
        "  else:\n",
        "    pace.append({\"model_name\": upscale_model, \"guided\": True, \"mag_adjust\": 1})\n",
        "  init_image = f\"{outputs_path}/{taskname_}_0_N.jpg\"\n",
        "  step = upscale_steps \n",
        "  starting_timestep = upscale_starting_timestep\n",
        "  w,h = w*multiply_image_size_by,h*multiply_image_size_by\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  eta_index=eta_schedule_proportional(eta_index)\n",
        "  for cc in [6]:\n",
        "        for bsq_scale in [.1]:\n",
        "              for grad_scale in [.1]:\n",
        "                 for active_function in [\"softsign\"]:\n",
        "                    torch.manual_seed(seed)\n",
        "                    random.seed(seed)\n",
        "                    if grad_scale!=1 and active_function==\"NA\": continue\n",
        "                    title2 = title + str(int(time.time()))\n",
        "                    taskname_ = title2 +\"_cc\"+str(cc)+\"_gs\"+str(grad_scale)#+ prompts[0]\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    do_run()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7kZUzvcoe7FB"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown ### Save current settings\n",
        "#@markdown If you would like to save your current settings, uncheck `skip_saving` and run this cell. You will get a `v_majesty_custom_settings.cfg` file you can reuse and share. If you like your results, send us a <a href=\"#\">pull request</a> to add your settings to the selectable library\n",
        "skip_saving = True #@param{type:'boolean'}\n",
        "if(not skip_saving):\n",
        "  data = generate_settings_file(add_prompts=False, add_dimensions=True)\n",
        "  text_file = open(\"v_majesty_custom_settings.cfg\", \"w\")\n",
        "  text_file.write(data)\n",
        "  text_file.close()\n",
        "  from google.colab import files\n",
        "  files.download('v_majesty_custom_settings.cfg')\n",
        "  #print(data)\n",
        "  print(\"Downloaded as custom_settings.cfg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgpcbjhD0vr0"
      },
      "source": [
        "### Biases acknowledgment\n",
        "Despite how impressive being able to turn text into image is, beware to the fact that this model may output content that reinforces or exarcbates societal biases. According to the <a href='https://arxiv.org/abs/2112.10752' target='_blank'>Latent Diffusion paper</a>:<i> \\\"Deep learning modules tend to reproduce or exacerbate biases that are already present in the data\\\"</i>. \n",
        "\n",
        "The models were trained on mostly non-curated image-text-pairs from the internet (the exception being the the removal of illegal content) and is meant to be used for research purposes, such as this one"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Rybdftj4B9G_",
        "HLQN0NTcCUtS",
        "g6coIeiGWWca"
      ],
      "name": "V-Majesty Diffusion",
      "private_outputs": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "9e3236fd3f990fb8325876dc599ef7209db0a0fa116cc6a98c96d08faccfbfa5"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('multimodal')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "9e3236fd3f990fb8325876dc599ef7209db0a0fa116cc6a98c96d08faccfbfa5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}