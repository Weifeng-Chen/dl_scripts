{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q67PoLPaHJRL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "# Stability.AI Easy Diffusion v0.16 ![visitors](https://visitor-badge.glitch.me/badge?page_id=EasyDiffusion&left_color=blue&right_color=orange) [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/WASasquatch/easydiffusion)\n",
        "\n",
        "Easy Diffusion was originally a fork of NOP's notebook, but has sort of evolved into it's own thing with many features. Such as depth output for 3D Facebook images, or post processing such as Depth of Field.\n",
        "\n",
        "If you'd like to help support the project and my time, feel free to buy me some bandwidth (I live rural and pay for bandwidth): https://paypal.me/ThompsonJordan\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "## <font color=\"default\">**Menu**</font>\n",
        "- <a href=\"#changelog\">**Change Log**</a>\n",
        "- <a href=\"#gpustatus\">**Check GPU Status**</a>\n",
        "- #### <a href=\"#setupenv\">**Setup Environment**</a>\n",
        " - <a href=\"#googledrive\">**Google Drive Options**</a>\n",
        " - <a href=\"#optionalfeats\">**Install Optional Features**</a>\n",
        " - <a href=\"#otherinstall\">**Other Install Options**</a>\n",
        "- #### <a href=\"#settingsdiffuse\">**Settings & Diffuse**</a>\n",
        " - <a href=\"#promptsetup\">**Prompt Setup**</a>\n",
        " - <a href=\"#initsetup\">**Init Image Setup**</a>\n",
        " - <a href=\"#diffusionsettings\">**Diffusion Settings**</a>\n",
        " - <a href=\"#upscalers\">**Upscaling Setup**</a>\n",
        " - <a href=\"#imageprocessors\">**Image Processing Setup**</a>\n",
        "   - <a href=\"#sharpen\">**Sharpen Image**</a>\n",
        "   - <a href=\"#kromo\">**Kromo Chromatic Aberration**</a>\n",
        "   - <a href=\"#median\">**Median Filter Image**</a>\n",
        "   - <a href=\"#midas\">**MiDaS Depth Export**</a>\n",
        "   - <a href=\"#fdof\">**Fake Depth of Field Filter**</a>\n",
        "   - <a href=\"#tileable\">**Tileable Seamless Image**</a>\n",
        " - <a href=\"#clipinterrogator\">**CLIP Interrogator**</a>\n",
        " - <a href=\"#othersettings\">**Other Diffusion Settings**</a>\n",
        "- <a href=\"#cleanenv\">**Clean Environment Up**</a>\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "## Stablity.AI Model Terms of Use\n",
        "\n",
        "**By using this Notebook, you agree to the following Terms of Use, and license**\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license \n",
        "\n",
        "## Expand for Changelog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G76cGaiuGdjJ"
      },
      "source": [
        "\n",
        "\n",
        "## <a name=\"changelog\">Change Log</a>:\n",
        "- v0.1: Forked [NOP's Stable Diffusion Colab v0.23](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17?usp=sharing)\n",
        "  - Added File Prompts\n",
        "  - Added Noodle Soup Prompts\n",
        "- 8/25/2022) Added better image output display\n",
        "- 8/26/2022) Added `INIT_IMAGE` support\n",
        "  - Added basic image output option\n",
        "- 8/27/2022) Patched CodeFormer fidelity path bug\n",
        "- 8/27/2022) Various code tweaks (by plambe#5832)\n",
        "  - Download some of the dependencies to google drive if enabled \n",
        "    - For instance the stable diffusion model\n",
        "    - Also multiple of the git repos\n",
        "  - Replaced all `!` and `%` in code to make it more universal\n",
        "- 8/27/2022) Patch NSP Installation, changed paths for Stable Diffusion and output images. (by WAS#0263)\n",
        "- 8/27/2022) Organized and improved installations\n",
        "- 8/28/2022) Real-ESRGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) GFPGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) CodeFormer bug fix (by plambe#5832)\n",
        "- 8/28/2022) Added cached diffusion piping: This will speed up run performance (WAS#0263)\n",
        "  - Added `RECACHE_PIPES` option\n",
        "  - Added `INCREMENT_ITERATION_SEED` option\n",
        "  - Patched working directory path for non-gdrive installations\n",
        "  - Patched working directory path for pipe cache not found\n",
        "  - Patched CodeFormer Fidelity paths, again?\n",
        "  - Added pre-ESRGAN down scaling option for GFPGAN + Real-ESRGAN, and CodeFormer + Real-ESRGAN.\n",
        "  - Added post diffusion sharpen option\n",
        "- **V0.6** | 8/29/2022) Added optional cached pipes. Using cached pipes is best for a high VRAM environment\n",
        "  - Added Kromo's Chromatic Aberration\n",
        "  - Added Sharpening\n",
        "  - Added optional dependency installs (save some space!)\n",
        "- **v0.7** | 8/29/2022) Added MiDaS Depth Approximation\n",
        "  - Depth maps can be used to apply Depth of Field, or other filmic effects in post processing with your favorite tools.\n",
        "- **v0.8** | 8/30/2022) Added Sampling Schedulers\n",
        "  -  Track function timing\n",
        "  - 8/31/2022) Patch MiDaS Depth Export even when unchecked.\n",
        "- **v0.9** | 9/1/2022) Added multi-init functionality to `INIT_IMAGE`.\n",
        "  - `INIT_IMAGE` supports a local/remote image path/url, a txt file containing a path/url per line, or a path to a folder containing images.\n",
        "  - Add `ESRGAN_MODE` which allows you to run ESRGAN on CPU if you want to conserve more VRAM.\n",
        "  - Add `MIDAS_PERSISTENT` mode. Keep MiDaS models in memory between iterations.\n",
        "  - 9/2/2022) Patch Pillow<9.0.0 versions for Resampling calls.\n",
        "  - Prepare for model selection\n",
        "- **v0.10** | 9/3/2022) Added collapsible batches and iterations in organized JS image output mode (default behavior)\n",
        "  - Improved environment cleanup (again)\n",
        "  - Added ability to clear log between iterations. *This would clear the diffusion result before viewing it in standard mode, so is disabled for non JS image output*\n",
        "- 9/4/2022) Add ability to skip diffusion run. Useful for processing prior diffusions or inits with image filters and upscalers. \n",
        "- **v0.11** | 9/5/2022) Patched img2img pipeline\n",
        " - Added Median Filter\n",
        " - Added Fake Depth of Field\n",
        " - Added Tileable Seamless Texture outout (also supports seamless depth map if Export Depth Map is enabled)\n",
        " - Various code improvements.\n",
        "- **v0.12** | 9/6/2022) Overhaul of Easy Diffusion Setup Process\n",
        " -  Easily enable/disable NSFW checker per run, and now returns a blurred image instead of black image. \n",
        " - CodeFormer now has a seprate upscale param, which unfortuantely couldn't be a slider. But CodeFormer now supports 1x (no upscaling).\n",
        " - Patch image display for FDOF active mode. \n",
        "- **v0.13** | 9/7/2022) Added Attention Slices optimization. This feature splits the job into slices for better use of available VRAM. Use `ENABLE_ATTENTION_SLICES` in diffusion settings to try it out.\n",
        " - `LOW_VRAM_PATCH` can be toggled from diffusion settings for **non-cached** pipes. \n",
        " - Add ability to set `MAX_SEED` size. Either a custom integether or `'system_max'` (with semi-quotes).\n",
        " - Add text2seed ability. Seed is now a raw input, and you can input text like `'rockycanyon'` (with semi-quotes) which will be converted to textual equivalent. \n",
        "- **v0.14** | 9/8/2022) Added ability to save model to Google Drive if not using drive for local copies.\n",
        " - Add `CUSTOM_MODEL` path field. This will override the selected `MODEL_ID` if exists.\n",
        " - Pipelines have been overhauled and should be much lighter on memory (should be the same as a regular loaded pipe), and offer much faster load times.\n",
        "- **v0.15** | 9/10/2022) Added Menu's for quicker initial navigation. \n",
        " - Added Google Colab paramScraper for settings export. \n",
        "- **v0.16** | 9/11/2022) Added pharmapsychotic's CLIP Interrogator\n",
        " - Interrogate init images or diffusion result\n",
        " - Model options are now available under `MODEL_ID`\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOEF-K5F5db"
      },
      "source": [
        "# Easy Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"gpustatus\">Check GPU Status</a>\n",
        "#@markdown Check the status of the allocated GPU\n",
        "import subprocess\n",
        "print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "nvidiasmi_simple = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "gpu_name = nvidiasmi_simple.split(':')[1].split('(')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P8gV4-qRDn1b"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"setupenv\"><font size=\"5\" color=\"default\">**Setup Environment**</font></a>\n",
        "\n",
        "# Import future print\n",
        "from __future__ import print_function\n",
        "try:\n",
        "    import __builtin__\n",
        "except ImportError:\n",
        "    import builtins as __builtin__\n",
        "\n",
        "# Emoticon fun!\n",
        "import subprocess\n",
        "try:\n",
        "    import emoji\n",
        "except ImportError:\n",
        "     multipip_res = subprocess.run(['pip', '-q', 'install', 'emoji'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "finally:\n",
        "    import emoji\n",
        "\n",
        "print(subprocess.run('python -m ensurepip --upgrade'.split(' '), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "# Override Print Function\n",
        "def print(message, *args, **kwargs):\n",
        "    if 'defaultprint' in kwargs:\n",
        "        kwargs.pop('defaultprint')\n",
        "        return __builtin__.print(message, *args, **kwargs)\n",
        "    else:\n",
        "        return __builtin__.print(emoji.emojize(message), *args, **kwargs)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"googledrive\">**Google Drive Options**</a>\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Use Google Drive to store images and prompt information</font>\n",
        "USE_DRIVE_FOR_LOCAL_COPIES = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Use Google Drive to store local copies of git repos, models and other assets</font><br>\n",
        "#@markdown <font size=\"3\" color=\"orange\">**WARNING:**</font> Requires 14gb+ of space (not including images produced). May not be suitable for Free Google Drive accounts.</font><br>\n",
        "#@markdown <font size=\"3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you encounter issues loading pipes, or Upscalers, you're likely out of storage space.</font>\n",
        "USE_DRIVE_FOR_MODELS = True #@param{type:'boolean'}\n",
        "#@markdown Use Google Drive for storing Stable Diffusion models. Only applicable if `USE_DRIVE_FOR_LOCAL_COPIES` is `False`.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"optionalfeats\">**Install Optional Features**</a>\n",
        "INSTALL_GFPGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install GFPGAN Face Enhancement</font>\n",
        "INSTALL_CODEFORMER = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install CodeFormer Face Enhancement</font>\n",
        "INSTALL_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install Real-ESRGAN Super Resolution</font>\n",
        "INSTALL_KROMO = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install Kromo Chromatic Aberration gnerator</font>\n",
        "INSTALL_MIDAS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Install timm for MiDaS support (this allows you to export Depth Maps)</font>\n",
        "INSTALL_IMG2TEXTURE = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Install img2texture for tileable seamless texture output.\n",
        "INSTALL_CLIP_INTERROGATOR = True #@param{type: 'boolean'}\n",
        "#@markdown Install CLIP Interrogator by pharmapsychotic\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"otherinstall\">**Other Install Options**</a>\n",
        "CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Clear the setup log after installation completes.</font>\n",
        "SUPPRESS_WARNINGS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Supress warnings from installation scripts and runtime scripts.</font>\n",
        "\n",
        "import os, sys, time, torch, gc, requests, io, shutil, json\n",
        "\n",
        "settings_template = {\n",
        "    'setup': {\n",
        "        'USE_DRIVE_FOR_PICS': False,\n",
        "        'USE_DRIVE_FOR_LOCAL_COPIES': False,\n",
        "        'USE_DRIVE_FOR_MODELS': False,\n",
        "        'INSTALL_GFPGAN': False,\n",
        "        'INSTALL_CODEFORMER': False,\n",
        "        'INSTALL_ESRGAN': False,\n",
        "        'INSTALL_KROMO': False,\n",
        "        'INSTALL_MIDAS': False,\n",
        "        'INSTALL_IMG2TEXTURE': False,\n",
        "        'CLEAR_SETUP_LOG': False,\n",
        "        'SUPPRESS_WARNINGS': True,\n",
        "    },\n",
        "    'prompts': {\n",
        "        'PROMPT': None,\n",
        "        'PROMPT_FILE': None,\n",
        "        'NEW_NSP_ON_ITERATION': True,\n",
        "    },\n",
        "    'inits': {\n",
        "        'INIT_IMAGE': None,\n",
        "        'INIT_SCALE': None,\n",
        "    },\n",
        "    'diffusion_settings': {\n",
        "        'MODEL_ID': None,\n",
        "        'SAMPLER': None,\n",
        "        'DDIM_ETA': None,\n",
        "        'STEPS': None,\n",
        "        'SEED': None,\n",
        "        'MAX_SEED': None,\n",
        "        'INCREMENT_ITERATION_SEED': None,\n",
        "        'NUM_ITERS': None,\n",
        "        'WIDTH': None,\n",
        "        'HEIGHT': None,\n",
        "        'SCALE': None,\n",
        "        'PRECISION': None,\n",
        "        'IMAGES_FOLDER': None,\n",
        "        'CACHE_PIPELINES': False,\n",
        "        'RECACHE_PIPES': False,\n",
        "        'SKIP_DIFFUSION_RUN': False,\n",
        "        'ENABLE_NSFW_FILTER': False,\n",
        "        'ENABLE_ATTENTION_SLICES': True,\n",
        "        'LOW_VRAM_PATCH': True,\n",
        "    },\n",
        "    'upscalers': {\n",
        "        'IMAGE_UPSCALER': None,\n",
        "        'UPSCALE_AMOUNT': None,\n",
        "        'ESRGAN_MODE': None,\n",
        "        'CODEFORMER_UPSCALE_AMOUNT': None,\n",
        "        'CODEFORMER_FIDELITY': None,\n",
        "    },\n",
        "    'image_processing': {\n",
        "        'SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN': True,\n",
        "        'SHARPEN_AMOUNT': None,\n",
        "        'CA_DIFFUSE_IMAGE': False,\n",
        "        'CA_STRENGTH': None,\n",
        "        'CA_JITTER': None,\n",
        "        'CA_OVERLAY': None,\n",
        "        'CA_NO_RADIAL_BLUR': None,\n",
        "        'MEDIAN_FILTER_IMAGE': False,\n",
        "        'MEDIAN_DIAMETER': None,\n",
        "        'MEDIAN_SIGMA_COLOR': None,\n",
        "        'MEDIAN_SIGMA_SPACE': None,\n",
        "        'EXPORT_MIDAS_DEPTH': False,\n",
        "        'MIDAS_TYPE': None,\n",
        "        'MIDAS_MODE': None,\n",
        "        'FDOF_IMAGE': False,\n",
        "        'FDOF_REPLACE_IMAGE': False,\n",
        "        'FDOF_RADIUS': None,\n",
        "        'FDOF_SAMPLES': None,\n",
        "        'TILEABLE_IMAGE': False,\n",
        "        'TILED': True,\n",
        "        'TILE_OVERLAP': None,\n",
        "    },\n",
        "    'clip_interrogator': {\n",
        "        'INTERROGATE_INIT_IAMGE': False,\n",
        "        'INTERROGATE_DIFFUSION_IMAGE': False,\n",
        "        'INTERROGATOR_MODE': None,\n",
        "        'ViTB32': False,\n",
        "        'ViTB16;': False,\n",
        "        'ViTL14': True,\n",
        "        'ViTL14_336px': True,\n",
        "        'RN101': False,\n",
        "        'RN50': False,\n",
        "        'RN50x4': False,\n",
        "        'RN50x16': False,\n",
        "        'RN50x64': False,\n",
        "        'INTERROGATOR_PROMPT': None,\n",
        "    },\n",
        "    'other_settings': {\n",
        "        'IMAGES_DISPLAY_ABOVE_LOG': False,\n",
        "        'USE_BASIC_IMAGE_DISPLAY': True,\n",
        "        'CLEAR_LOG_BETWEEN_ITERATIONS': True,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# SETUP BASE DIRECTORIES\n",
        "OUTDIR = '/content/Stable_Diffusion/images_out'\n",
        "\n",
        "if USE_DRIVE_FOR_MODELS and USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    USE_DRIVE_FOR_MODELS = False\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "STABLE_DIFFUSION_WORKDIR = '/content/Stable_Diffusion'\n",
        "GDRIVE_WORKDIR = '/content/drive/MyDrive/AI/Stable_Diffusion'\n",
        "\n",
        "if USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    STABLE_DIFFUSION_WORKDIR = GDRIVE_WORKDIR\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "if not USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "sys.path.append(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "drive_model_cache = f'{GDRIVE_WORKDIR}/model_cache'\n",
        "model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "moved_from_cache = False\n",
        "last_model = None\n",
        "\n",
        "if not os.path.exists(model_cache):\n",
        "    os.makedirs(model_cache)\n",
        "\n",
        "# DEFINE NECESSARY FUNCTIONS\n",
        "\n",
        "def packages():\n",
        "    import sys, subprocess\n",
        "    return [r.decode().split('==')[0] for r in subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).split()]\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', '-q', '--show-progress', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wgeto(url, outputdir):\n",
        "    res = subprocess.run(['wget', '-q', '--show-progress', url, '-O', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def plotSettings(settingsType=None, locals=None):\n",
        "    global settings\n",
        "    if settingsType and settings.__contains__(settingsType) and type(locals) is dict:\n",
        "        for k in settings[settingsType].keys():\n",
        "            if locals.keys().__contains__(k):\n",
        "                settings[settingsType][k] = locals[k]\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def clear():\n",
        "    from IPython.display import clear_output; return clear_output()\n",
        "\n",
        "def time_format(seconds: int):\n",
        "    if seconds is not None:\n",
        "        seconds = int(seconds)\n",
        "        d = seconds // (3600 * 24)\n",
        "        h = seconds // 3600 % 24\n",
        "        m = seconds % 3600 // 60\n",
        "        s = seconds % 3600 % 60\n",
        "        ms = round(seconds * 1000)\n",
        "        if d > 0:\n",
        "            return '{:02d}D {:02d}H {:02d}m {:02d}s'.format(d, h, m, s)\n",
        "        elif h > 0:\n",
        "            return '{:02d}H {:02d}m {:02d}s'.format(h, m, s)\n",
        "        elif m > 0:\n",
        "            return '{:02d}m {:02d}s'.format(m, s)\n",
        "        elif s > 0:\n",
        "            return '{:02d}s'.format(s)\n",
        "        elif ms > 0:\n",
        "            return '{:02d}ms'.format(ms)\n",
        "    return '0s'\n",
        "\n",
        "def text2seed(string, max):\n",
        "    seed = None\n",
        "    def digits(n, max):\n",
        "        import math\n",
        "        ndigits = int(math.log10(n))+1\n",
        "        try:\n",
        "            return n//int(10**(ndigits-max))\n",
        "        except ZeroDivisionError:\n",
        "            return n\n",
        "    if string:\n",
        "        for chr in [*string]:\n",
        "            if seed is None:\n",
        "                seed = str(ord(chr))\n",
        "            else:\n",
        "                seed += str(ord(chr))\n",
        "        seed = digits(int(seed), max)\n",
        "    return seed\n",
        "\n",
        "def gpu_memory_usage(gpu_id):\n",
        "    command = f\"nvidia-smi --id={gpu_id} --query-gpu=memory.used --format=csv\"\n",
        "    output_cmd = subprocess.check_output(command.split())\n",
        "    memory_used = output_cmd.decode(\"ascii\").split(\"\\n\")[1]\n",
        "    memory_used = int(memory_used.split()[0])\n",
        "    return memory_used\n",
        "\n",
        "def gpu_memory_total(gpu_id):\n",
        "    command = f\"nvidia-smi --id={gpu_id} --query-gpu=memory.total --format=csv\"\n",
        "    output_cmd = subprocess.check_output(command.split())\n",
        "    memory_used = output_cmd.decode(\"ascii\").split(\"\\n\")[1]\n",
        "    memory_used = int(memory_used.split()[0])\n",
        "    return memory_used\n",
        "\n",
        "def clean_env(v=False, device=0):\n",
        "    import time\n",
        "    cuda_availabe = torch.cuda.is_available()\n",
        "    mem_used = gpu_memory_usage(device)\n",
        "    mem_total = gpu_memory_total(device)\n",
        "    if v: print(f'VRAM Total: {mem_total}mb, VRAM Allocatd: {mem_used}mb')\n",
        "    stt = int(time.time())\n",
        "    if cuda_availabe:\n",
        "        a = None\n",
        "        try:\n",
        "            a = torch.zeros(sys.maxsize, dtype=torch.int8).cuda()\n",
        "        except Exception:\n",
        "            pass\n",
        "        finally:\n",
        "            del a\n",
        "            torch.cuda.synchronize(); \n",
        "            torch.cuda.empty_cache(); \n",
        "    time.sleep(0.25)\n",
        "    gc.collect()\n",
        "    try:\n",
        "        global midas, transform, prediction, input_batch, depth, depth_image, image, sr_image, enhanced_image, img, init,  original_init\n",
        "        del midas, transform, prediction, input_batch, depth, depth_image, image, sr_image, enhanced_image, img, init,  original_init\n",
        "    except NameError:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "    if v: print(f':recycling_symbol: Cleared memory.  Time taken was {time_format(int(int(time.time()) - stt))}')\n",
        "    new_mem_used = gpu_memory_usage(device)\n",
        "    if v: print(f'VRAM Allocatd: {new_mem_used}mb, VRAM Released: {mem_used - new_mem_used}mb')\n",
        "    if not cuda_availabe:\n",
        "        print(\":WARNING: There is no CUDA device available! Cannot run diffusion models!\")\n",
        "\n",
        "\n",
        "# Basic image display -- God, what is this monster I've spawned? \n",
        "def displayJsImage(b, i, prepend, name, img):\n",
        "    import cv2\n",
        "    from IPython.display import display, Javascript\n",
        "    from google.colab.output import eval_js\n",
        "    from base64 import b64encode\n",
        "    from google.colab import files\n",
        "    import numpy as np\n",
        "    img = np.asarray(img, dtype=np.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    js = Javascript('''\n",
        "        async function showImage(b, i, prepend, name, image, width, height) {\n",
        "            batchBlock = document.getElementById('batch-block-'+b);\n",
        "            block = document.getElementById('block-'+b+'-'+i)\n",
        "            img = document.getElementById(name);\n",
        "            cont = document.getElementById(name+'_container');\n",
        "\n",
        "            if (batchBlock == null) {\n",
        "                batchBlock = document.createElement('div');\n",
        "                batchBlock.id = 'batch-block-'+b;\n",
        "                batchBlock.style = 'background-color:rgba(0,0,0,0.25);width:auto;margin-bottom:25px;padding:5px;text-align:center;box-shadow: 0px 0px 5px rgba(0,0,0,0.5);';\n",
        "                //batchBlock.innerHTML = '<h2 style=\"background-color:rgba(255,255,255,0.1);margin:0;margin-bottom:5px;padding:4px;text-align:center;text-shadow: 1px 1px rgba(0,0,0,0.35);\">Batch '+b+'</h2>';\n",
        "                if (prepend == 1) {\n",
        "                    document.body.prepend(batchBlock)\n",
        "                } else {\n",
        "                    document.body.appendChild(batchBlock)\n",
        "                }\n",
        "                buttonBt = document.createElement('button');\n",
        "                buttonBt.className = 'collapsible';\n",
        "                buttonBt.style = 'cursor:pointer;width:100%;margin-bottom:5px;border:none;border-bottom:3px solid #999999;padding:5px;text-align:center;font-size:16px;font-weight:bold;color:white;background-color:rgba(155,155,155,0.15);text-shadow: 1px 1px rgba(0,0,0,0.35);transition: all 0.5s;'\n",
        "                buttonBt.innerHTML = 'Batch '+b;\n",
        "                buttonBt.value = 'Batch '+b;\n",
        "                batchBlock.before(buttonBt)\n",
        "            }\n",
        "            if (block == null) {\n",
        "                block = document.createElement('div');\n",
        "                block.id = 'block-'+b+'-'+i;\n",
        "                block.style = 'width: auto;margin-bottom:15px;padding:5px;text-align:center;';\n",
        "                //block.innerHTML = '<h3 style=\"margin:3px;text-align:center;text-shadow: 1px 1px rgba(0,0,0,0.35);\">Iteration '+i+'</h3>';\n",
        "                batchBlock.appendChild(block);\n",
        "                buttonIt = document.createElement('button');\n",
        "                buttonIt.className = 'collapsible';\n",
        "                buttonIt.style = 'cursor:pointer;width:100%;margin-bottom:5px;border:none;border-bottom:3px solid #999999;padding:5px;text-align:center;font-size:16px;font-weight:bold;color:white;background-color:rgba(155,155,155,0.15);text-shadow: 1px 1px rgba(0,0,0,0.35);transition: all 0.5s;'\n",
        "                buttonIt.innerHTML = 'Iteration '+i;\n",
        "                buttonIt.value = 'Iteration '+i;\n",
        "                block.before(buttonIt)\n",
        "            }\n",
        "            if(img == null && cont == null) {\n",
        "                cont = document.createElement('div');\n",
        "                cont.id = name+'_container';\n",
        "                link = document.createElement('a');\n",
        "                link.href = image;\n",
        "                link.target = '_blank';\n",
        "                img = document.createElement('img');\n",
        "                img.id = name;\n",
        "                img.class = \"resultImage\"\n",
        "                cont.style = 'display:inline-block;width:auto;font-size:14px;font-weight:bold;background-color:rgba(0,0,0,0.5);border-radius:5px;padding:2px;margin:2px;box-shadow: 0px 0px 5px rgba(0,0,0,0.5);'\n",
        "                cont.innerHTML = '<p style=\"margin:3px auto;width:180px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-shadow: 1px 1px rgba(0,0,0,0.35);\">'+name+'</p>';\n",
        "                block.appendChild(cont);\n",
        "                cont.appendChild(link);\n",
        "                link.appendChild(img);\n",
        "            }\n",
        "            img.src = image;\n",
        "            img.style = \"margin: 5px; vertical-align: text-top; max-width: 256px; max-height: 512px;\";\n",
        "        }\n",
        "\n",
        "        var coll = document.getElementsByClassName(\"collapsible\");\n",
        "        var i;\n",
        "        for (i = 0; i < coll.length; i++) {\n",
        "\n",
        "            coll[i].addEventListener('mouseover',function(){\n",
        "                this.style.color = 'orange';\n",
        "                this.style.borderBottom = \"3px solid orange\";\n",
        "            })\n",
        "\n",
        "            coll[i].addEventListener('mouseleave',function(){\n",
        "                this.style.color = 'white';\n",
        "                this.style.borderBottom = \"3px solid #999\";\n",
        "            })\n",
        "\n",
        "            coll[i].addEventListener(\"click\", function() {\n",
        "                this.classList.toggle(\"active\");\n",
        "                var content = this.nextElementSibling;\n",
        "                if (content.style.display === \"block\") {\n",
        "                content.style.display = \"none\";\n",
        "                } else {\n",
        "                content.style.display = \"block\";\n",
        "                }\n",
        "            });\n",
        "\n",
        "        }\n",
        "    ''')\n",
        "    height, width = img.shape[:2]\n",
        "    ret, data = cv2.imencode('.png', img)\n",
        "    data = b64encode(data)\n",
        "    data = data.decode()\n",
        "    data = 'data:image/png;base64,' + data\n",
        "    display(js)\n",
        "    eval_js(f'showImage({b}, {i}, {int(prepend)}, \"{name}\", \"{data}\", {width}, {height})')\n",
        "\n",
        "def clearOutputArea(b, i):\n",
        "    from IPython.display import display, Javascript\n",
        "    from google.colab.output import eval_js\n",
        "    js = Javascript('''\n",
        "        function onReady(fn) {\n",
        "            if (document.readyState==='complete' || document.readyState==='interactive') {\n",
        "                setTimeout(fn, 1);\n",
        "            } else {\n",
        "                document.addEventListener(\"DOMContentLoaded\", fn);\n",
        "            }\n",
        "        }\n",
        "        function clearColabOutput(b, i) {\n",
        "            var streams = document.getElementsByClassName('stream');\n",
        "            var dataOutputs = document.getElementsByClassName('display_data');\n",
        "            for(var i = 0; i < streams.length; i++) {\n",
        "                streams[i].innerHTML = '';\n",
        "            }\n",
        "            for(var i = 0; i < dataOutputs.length; i++) {\n",
        "                dataOutputs[i].innerHTML = ''\n",
        "            }\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    eval_js(f'onReady(clearColabOutput({b}, {i}));')\n",
        "\n",
        "def closest_value(input_list, input_value):\n",
        "    difference = lambda input_list : abs(input_list - input_value)\n",
        "    res = min(input_list, key=difference)\n",
        "    return res\n",
        "\n",
        "def printPrompt(prompt, limit=12):\n",
        "    pw = prompt.split(\" \"); i=0; oi=0; pstr = ''\n",
        "    for w in pw:\n",
        "        oi+=1; pstr += f'{w} '\n",
        "        if i is limit or oi is len(pw): print(pstr.strip()); pstr = ''; i = 0; pass\n",
        "        i+=1\n",
        "\n",
        "def sharpenImage(image, samples=1):\n",
        "    import PIL\n",
        "    from PIL import Image, ImageFilter\n",
        "    im = image\n",
        "    for i in range(samples):\n",
        "        im = im.filter(ImageFilter.SHARPEN)\n",
        "    return im\n",
        "\n",
        "def medianFilter(img, diameter, sigmaColor, sigmaSpace):\n",
        "    from PIL import Image\n",
        "    import cv2 as cv\n",
        "    import numpy as np\n",
        "    diameter = int(diameter); sigmaColor = int(sigmaColor); sigmaSpace = int(sigmaSpace)\n",
        "    img = img.convert('RGB')\n",
        "    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)\n",
        "    img = cv.bilateralFilter(img, diameter, sigmaColor, sigmaSpace)\n",
        "    img = cv.cvtColor(np.array(img), cv.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img).convert('RGB')\n",
        "\n",
        "def portraitBlur(img, mask, radius=5, samples=1):\n",
        "    from PIL import Image, ImageFilter\n",
        "    mask = mask.resize(img.size).convert('L')\n",
        "    #bimg = img.filter(ImageFilter.BoxBlur(int(boxBlur)))\n",
        "    bimg = medianFilter(img, radius, (radius * 500), 75)\n",
        "    bimg.convert(img.mode)\n",
        "    rimg = None\n",
        "    if samples > 1:\n",
        "        for i in range(samples):\n",
        "            if i is 0:\n",
        "                rimg = Image.composite(img, bimg, mask)\n",
        "            else:\n",
        "                rimg = Image.composite(rimg, bimg, mask)\n",
        "    else:\n",
        "        rimg = Image.composite(img, bimg, mask).convert('RGB')\n",
        "    \n",
        "    return rimg\n",
        "\n",
        "def getInitImages(path, verbose=False):\n",
        "    ret_images = []\n",
        "    valid = ['.jpeg','.jpg','.gif','.png']\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        if verbose: print(f'Found 1 remote image: {path}\\n')\n",
        "        return path\n",
        "    if os.path.isdir(path):\n",
        "        try:\n",
        "            images = next(os.walk(path), (None, None, []))[2]\n",
        "            ret_images = []\n",
        "            if images:\n",
        "                if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                for img in images:\n",
        "                    ext = os.path.splitext(img)[1]\n",
        "                    if ext in valid:\n",
        "                        img = f'{path}/{img}'\n",
        "                        if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                        ret_images.append(img)\n",
        "                print('')\n",
        "            if len(ret_images) == 0:\n",
        "                if verbose: print(f'Found no valid image(s)\\n')\n",
        "                return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    elif os.path.isfile(path):\n",
        "        try:\n",
        "            if path.lower().endswith('.txt'):\n",
        "                with open(path, \"r\") as f:\n",
        "                    images = f.read().splitlines()\n",
        "                    if images:\n",
        "                        ret_images = []\n",
        "                        if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                        for img in images:\n",
        "                            ext = os.path.splitext(img)[1]\n",
        "                            if ext in valid:\n",
        "                                if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                                ret_images.append(img)\n",
        "                        print('')\n",
        "            else:\n",
        "                ext = os.path.splitext(path)[1]\n",
        "                if ext.lower() in valid:\n",
        "                    if verbose: print(f'Found 1 image: {path}\\n')\n",
        "                    return path\n",
        "                else:\n",
        "                    if verbose: print(f'Found no valid image(s)\\n')\n",
        "                    return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    return ret_images\n",
        "\n",
        "def setup_pipes(pipe_type='default', model_id=None, model_cache=None,):\n",
        "    if pipe_type is 'lowvram':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        del pipe.vae.encoder\n",
        "    elif pipe_type is 'img2img':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    elif pipe_type is 'default':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\")\n",
        "    return pipe\n",
        "\n",
        "def cache_pipes(model_id, model_cache, pipe_cache):\n",
        "    global RECACHE_PIPES\n",
        "    import joblib, os\n",
        "    from diffusers import StableDiffusionImg2ImgPipeline    \n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "    if LOW_VRAM_PATCH:\n",
        "        if not os.path.exists(f'{pipe_cache}/LOW_VRAM_PIPE.obj') or RECACHE_PIPES:\n",
        "            joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/LOW_VRAM_PIPE.obj')\n",
        "            #del piping['LOW_VRAM'].vae.encoder\n",
        "            clean_env()\n",
        "            if os.path.exists(f'{pipe_cache}/LOW_VRAM_PIPE.obj'):\n",
        "                print('Cached pipe:', f'{pipe_cache}/LOW_VRAM_PIPE.obj')\n",
        "    if not os.path.exists(f'{pipe_cache}/IMG2IMG_PIPE.obj') or RECACHE_PIPES:\n",
        "        joblib.dump(StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/IMG2IMG_PIPE.obj')\n",
        "        if os.path.exists(f'{pipe_cache}/IMG2IMG_PIPE.obj'):\n",
        "            print('Cached pipe:', f'{pipe_cache}/IMG2IMG_PIPE.obj')\n",
        "        clean_env()\n",
        "    if not os.path.exists(f'{pipe_cache}/DEFAULT.obj') or RECACHE_PIPES:\n",
        "        joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/DEFAULT_PIPE.obj')\n",
        "        if os.path.exists(f'{pipe_cache}/DEFAULT.obj'):\n",
        "            print('Cached pipe:', f'{pipe_cache}/DEFAULT.obj')\n",
        "        clean_env()\n",
        "\n",
        "def safetyCheckerDummy(images, **kwargs):\n",
        "    return images, False\n",
        "\n",
        "def preprocess(image):\n",
        "    import PIL\n",
        "    import numpy as np\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "# Optimization Functions\n",
        "\n",
        "def forward(self, x, context=None, mask=None):\n",
        "\n",
        "    import math\n",
        "    from torch import einsum\n",
        "    import subprocess\n",
        "    try:\n",
        "      from einops import rearrange\n",
        "    except ModuleNotFoundError:\n",
        "      subprocess.run(['pip', 'install', 'einops'], stdout=subprocess.DEVNULL)\n",
        "      from einops import rearrange\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    import torch\n",
        "    batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = context if context is not None else x\n",
        "    k = self.to_k(context)\n",
        "    v = self.to_v(context)\n",
        "    del context, x\n",
        "\n",
        "    q = self.reshape_heads_to_batch_dim(q)\n",
        "    k = self.reshape_heads_to_batch_dim(k)\n",
        "    v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "    stats = torch.cuda.memory_stats(q.device)\n",
        "    mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "    mem_active = stats['active_bytes.all.current']\n",
        "    mem_free = mem_total - mem_active\n",
        "\n",
        "    mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "    steps = 1\n",
        "\n",
        "    if mem_required > mem_free:\n",
        "        steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "    slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "    for i in range(0, q.shape[1], slice_size):\n",
        "        end = i + slice_size\n",
        "        s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "        s1 *= self.scale\n",
        "\n",
        "        s2 = s1.softmax(dim=-1)\n",
        "        del s1\n",
        "\n",
        "        r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "        del s2\n",
        "\n",
        "    del q, k, v\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "def optimize_attention(model):\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = types.MethodType(forward, module)\n",
        "\n",
        "# End Optimization Functions\n",
        "\n",
        "def move_files(source, destination):\n",
        "    for src_dir, dirs, files in os.walk(source):\n",
        "        dst_dir = src_dir.replace(source, destination)\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.mkdir(dst_dir)\n",
        "        for file_ in files:\n",
        "            src_file = os.path.join(src_dir, file_)\n",
        "            dst_file = os.path.join(dst_dir, file_)\n",
        "            if os.path.exists(dst_file):\n",
        "                os.remove(dst_file)\n",
        "            shutil.copy(src_file, dst_dir)\n",
        "\n",
        "def download_model(model_id, redownload=False):\n",
        "    import os, shutil\n",
        "    global moved_from_cache, last_model\n",
        "    rep = ['CompVis/','hakurei/']\n",
        "    localf = model_id\n",
        "    for r in rep:\n",
        "        localf = model_id.replace(r, '')\n",
        "    model = f'{model_cache}/{localf}'\n",
        "    if redownload and moved_from_cache:\n",
        "        moved_from_cache = False\n",
        "    drive_model = f'{drive_model_cache}/{localf}'\n",
        "    if USE_DRIVE_FOR_MODELS and not USE_DRIVE_FOR_LOCAL_COPIES and not moved_from_cache and not redownload:\n",
        "        print(\":open_file_folder: Moving model files to model cache from drive cache ...\")\n",
        "        if not os.path.exists(model_cache):\n",
        "            os.makedirs(model_cache)\n",
        "        if os.path.exists(drive_model) or len(os.listdir(drive_model_cache)) > 0:\n",
        "            try:\n",
        "                move_files(drive_model_cache, model_cache)\n",
        "                print(\":check_mark_button: Move complete.\")\n",
        "                redownload = False\n",
        "                moved_from_cache = True\n",
        "            except OSError as e:\n",
        "                redownload = True\n",
        "                print(\"Uneable to move model cache from:\", drive_model_cache)\n",
        "                pass\n",
        "        else:\n",
        "            print(f':WARNING: \\'{model_id}\\' doesn\\'t exist in \\'{drive_model_cache}\\', or any other model weights or models!')\n",
        "            redownload = True\n",
        "    if redownload:\n",
        "        if os.path.exists(model):\n",
        "            shutil.rmtree(model)\n",
        "        os.chdir(model_cache)\n",
        "        print(\":hourglass_not_done: Downloading model weights for:\", model_id)\n",
        "        print(subprocess.run(['git', 'clone', f'https://{hu}:{ht}@huggingface.co/{model_id}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: Downloaded complete.\")\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    if USE_DRIVE_FOR_MODELS and not USE_DRIVE_FOR_LOCAL_COPIES and not moved_from_cache:\n",
        "        print(\":open_file_folder: Moving model files to drive cache ...\")\n",
        "        if not os.path.exists(drive_model_cache):\n",
        "            os.makedirs(drive_model_cache)\n",
        "        if os.path.exists(model) or len(os.listdir(model_cache)) > 0:\n",
        "            move_files(model_cache, drive_model_cache)\n",
        "            print(\":check_mark_button: Move complete.\")\n",
        "        else:\n",
        "            print(f':WARNING: \\'{model_id}\\' doesn\\'t exist in \\'{model_cache}\\', or any other model weights or models!')\n",
        "        moved_from_cache = True\n",
        "\n",
        "# End Optimization Functions\n",
        "\n",
        "# SETUP DEPENDENCIES\n",
        "print(\"\\nStarting Installation Processess.\\nThis should take approximately one eternity...\\n\")\n",
        "\n",
        "try:\n",
        "  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "    k = f.read().decode('utf-8').split(':'); hu = k[0].strip(); ht = k[1].strip()\n",
        "except OSError as e:\n",
        "  raise e\n",
        "\n",
        "try:\n",
        "\n",
        "    # Install psutil\n",
        "    if 'psutil' in packages():\n",
        "        print(':check_mark_button: \\'psutil\\' installed.\\n')\n",
        "    else:\n",
        "        print(':hourglass_not_done: Installing \\'psutil\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'psutil'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'psutil\\' installed.\\n')\n",
        "    import psutil\n",
        "\n",
        "    if 'joblib' in packages():\n",
        "        print(':check_mark_button: \\'joblib\\' installed.\\n')\n",
        "    else:\n",
        "        print(':hourglass_not_done: Installing \\'joblib\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'joblib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'joblib\\' installed.\\n')\n",
        "    import joblib\n",
        "    from joblib import Memory\n",
        "    cache_dir = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "\n",
        "    # Install Shutup\n",
        "    if 'shutup' not in packages():\n",
        "        subprocess.run(['pip', '-q', 'install', 'shutup'], stdout=subprocess.DEVNULL)\n",
        "    import shutup; \n",
        "    if SUPPRESS_WARNINGS: \n",
        "        shutup.please()\n",
        "\n",
        "    import warnings\n",
        "    if SUPPRESS_WARNINGS:\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "    \n",
        "    #rint(subprocess.run(['git', 'lfs', 'install'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    #os.environ['GIT_LFS_SKIP_SMUDGE'] = \"0\"\n",
        "\n",
        "    # This will take a while\n",
        "\n",
        "    # Install Diffusers\n",
        "    if 'diffusers' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'diffusers\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', '-U', 'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'diffusers\\' installed.\\n')\n",
        "        \n",
        "    # Download the model weights\n",
        "    #print(':hourglass_not_done: Downloading model weights ...')\n",
        "    #print(subprocess.run(['git', 'clone', '--quiet', f'https://{hu}:{ht}@huggingface.co/CompVis/stable-diffusion-v1-4'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    #print(':check_mark_button: Downloaded model weights.\\n')\n",
        "\n",
        "    # Download the model file\n",
        "    #if USE_DRIVE_FOR_MODELS and os.path.exists(f'{drive_model_cache}/sd-v1-4.ckpt') and not os.path.exists(f'{model_cache}/sd-v1-4.ckpt'):\n",
        "    #    print(':hourglass_not_done: Moving model from Google Drive to model cache...')\n",
        "    #    shutil.copy(f'{drive_model_cache}/sd-v1-4.ckpt', f'{model_cache}/sd-v1-4.ckpt')\n",
        "    #if not os.path.exists(f'{model_cache}/sd-v1-4.ckpt'):\n",
        "    #    print(':hourglass_not_done: Downloading sd-v1-4.ckpt model ...')\n",
        "    #    wget(f'https://{hu}:{ht}@huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', f'{model_cache}')\n",
        "    #    time.sleep(0.1)\n",
        "    #    if os.path.exists(f'{model_cache}/sd-v1-4.ckpt'):\n",
        "    #        print(':check_mark_button: sd-v1-4.ckpt model downloaded.')\n",
        "    #        if USE_DRIVE_FOR_MODELS:\n",
        "    #            shutil.copy(f'{model_cache}/sd-v1-4.ckpt', f'{drive_model_cache}/sd-v1-4.ckpt')\n",
        "    #    else:\n",
        "    #        print(':warning: Unable to download sd-v1-4.ckpt model!')\n",
        "\n",
        "    if os.path.exists('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py'):\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        wgeto('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/safety_checker.py', 'safety_checker.py')\n",
        "        shutil.copy('safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/')\n",
        "\n",
        "    if 'transformers' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'transformers\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'transformers\\' installed.\\n')\n",
        "\n",
        "    #model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "    print(':hourglass_not_done: Installing pytorch dependencies...')\n",
        "    res = ''\n",
        "    if 'pytorch-pretrained-bert' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'pytorch-pretrained-bert'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if 'spacy' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'spacy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if 'ftfy' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'ftfy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "    print(':check_mark_button: pytorch dependencies installed.\\n')\n",
        "\n",
        "    if 'spacy' not in packages():\n",
        "        print(':hourglass_not_done: Setting up \\'spacy\\' ...\\n')\n",
        "        if SUPPRESS_WARNINGS:\n",
        "            subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.DEVNULL)\n",
        "        else:\n",
        "            print(subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'spacy\\' setup complete.\\n')\n",
        "    else:\n",
        "        print(':check_mark_button: \\'spacy\\' installed.\\n')\n",
        "\n",
        "    if 'scipy' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'scipy\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(':check_mark_button: \\'scipy\\' installed.\\n')\n",
        "\n",
        "    if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/k-diffusion'):\n",
        "        print(':hourglass_not_done: Installing \\'k-diffusers\\' ...')\n",
        "        print(subprocess.run(['git', 'clone', '--quiet', '--recursive', 'https://github.com/crowsonkb/k-diffusion.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    else:\n",
        "        print(':check_mark_button: \\'k-diffusion\\' installed.\\n')\n",
        "\n",
        "    print(':globe_with_meridians: Logging into HuggingFace API...')\n",
        "    subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "    left_of_pipe = subprocess.Popen([\"echo\", ht], stdout=subprocess.PIPE)\n",
        "    right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(right_of_pipe)\n",
        "    \n",
        "    # Import Pipelines\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from diffusers import StableDiffusionImg2ImgPipeline    \n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "    if INSTALL_GFPGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing GFPGAN...\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            wget(\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\", \"experiments/pretrained_models\")\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            \n",
        "        if ['basicsr', 'facexlib'] not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'basicsr', 'facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if SUPPRESS_WARNINGS:\n",
        "                subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.DEVNULL)\n",
        "            else:\n",
        "                res += subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if 'realesrgan' not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "        print(\":check_mark_button: GFPGAN installed!\\n\")\n",
        "        \n",
        "    if INSTALL_ESRGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing Real-ESRGAN\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            res += subprocess.run(['pip', '-q', 'install', '-r', 'Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth\", \"Real-ESRGAN/weights/\")\n",
        "        print(res)\n",
        "        print(\":check_mark_button: Real-ESRGAN installed!\\n\")\n",
        "        \n",
        "        def upscale(image, scale, device='cuda'):\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            from realesrgan import RealESRGAN\n",
        "            device = torch.device(device)\n",
        "            model = RealESRGAN(device, scale = scale)\n",
        "            model.load_weights(f'weights/RealESRGAN_x{scale}.pth')\n",
        "            sr_image = model.predict(np.array(image))\n",
        "            del model, device\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            return sr_image\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        print(\":hourglass_not_done: Installing CodeFormer...\\n\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        if ['codeformer','CodeFormer'] not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            # Install basicsr\n",
        "            if SUPPRESS_WARNINGS:\n",
        "                subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.DEVNULL)\n",
        "            else:\n",
        "                res += subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "            # Download the pre-trained model\n",
        "            if SUPPRESS_WARNINGS:\n",
        "                subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.DEVNULL)\n",
        "                subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.DEVNULL)\n",
        "            else: \n",
        "                res += subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "                res += subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            print(res)\n",
        "            os.makedirs('temp', exist_ok=True)\n",
        "            os.makedirs('results', exist_ok=True)\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(\":check_mark_button: CodeFormer installed!\\n\")\n",
        "\n",
        "    if INSTALL_KROMO:\n",
        "        if 'kromo' not in packages():\n",
        "            print(\":hourglass_not_done: Installing \\'kromo\\' ...\")\n",
        "            res = ''\n",
        "            if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/kromo'):\n",
        "                os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "                res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/yoonsikp/kromo'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if 'kromo' not in packages():\n",
        "                os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "                res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "                os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(res)\n",
        "        print(':check_mark_button: \\'kromo\\' installed.\\n')\n",
        "\n",
        "    if INSTALL_MIDAS:\n",
        "        if 'timm' not in packages():\n",
        "            print(\":hourglass_not_done: Installing MiDaS compatibility...\")\n",
        "            print(subprocess.run(['pip', '-q', 'install', 'timm'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: MiDaS compatibility installed!\\n\")\n",
        "\n",
        "    if INSTALL_IMG2TEXTURE:\n",
        "        if 'img2texture' not in packages():\n",
        "            print(\":hourglass_not_done: Installing \\'img2texture\\' ...\")\n",
        "            print(subprocess.run(['pip3', '-q', 'install', 'img2texture'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: img2texture installed.\\n\")\n",
        "\n",
        "    # Colab Param Scraper\n",
        "    try:\n",
        "        from colabparamscraper.paramscraper import paramScraper\n",
        "    except ImportError:\n",
        "        print(\":hourglass_not_done: Installing Colab paramScraper ...\")\n",
        "        print(subprocess.run(['git', 'clone', '--quiet', 'https://github.com/WASasquatch/colabparamscraper'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    finally:\n",
        "        from colabparamscraper.paramscraper import paramScraper\n",
        "        print(\":check_mark_button: Colab paramScraper installed!\\n\")\n",
        "    \n",
        "    # Noodle Soup prompts\n",
        "    try:\n",
        "        import nsp_pantry\n",
        "    except ImportError:\n",
        "        if not os.path.exists('nsp_pantry.py'):\n",
        "            print(\":hourglass_not_done: Installing Noodle Soup Prompts...\")\n",
        "            wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py', './')\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nspterminology, nsp_parse\n",
        "\n",
        "    if nsp_parse and nspterminology:\n",
        "        print(\"\\r\\r:check_mark_button: \\33[32mNSP installed successfuly.\\33[0m \\x1B[3mMmm... Noodle Soup.\\x1B[0m\\n\")\n",
        "\n",
        "    # CLIP Interrogator\n",
        "    if INSTALL_CLIP_INTERROGATOR:\n",
        "\n",
        "        res = ''\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(\":hourglass_not_done: Installing CLIP Interrogator, and dependencies ...\")\n",
        "        if ['regex', 'tqdm', 'transformers', 'time', 'fairscale'] not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'regex', 'tqdm', 'transformers', 'timm', 'fairscale==0.4.4'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if 'clip' not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'git+https://github.com/openai/CLIP.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/clip-interrogator'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/pharmapsychotic/clip-interrogator.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/BLIP'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/salesforce/BLIP'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "\n",
        "        sys.path.append(f'{STABLE_DIFFUSION_WORKDIR}/BLIP')\n",
        "        interrogator_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        interrogator_device = 'cpu'\n",
        "\n",
        "        import clip\n",
        "        import pandas as pd\n",
        "        import requests\n",
        "        import torch\n",
        "        import torchvision.transforms as T\n",
        "        import torchvision.transforms.functional as TF\n",
        "\n",
        "        from IPython.display import display\n",
        "        from PIL import Image\n",
        "        from torch import nn\n",
        "        from torch.nn import functional as F\n",
        "        from torchvision import transforms\n",
        "        from torchvision.transforms.functional import InterpolationMode\n",
        "            \n",
        "        from BLIP.models.blip import blip_decoder\n",
        "        \n",
        "        #%cd /content/BLIP\n",
        "\n",
        "        os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE']='1'\n",
        "        #os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE=1']='1'\n",
        "\n",
        "        def generate_caption(pil_image):\n",
        "            gpu_image = transforms.Compose([\n",
        "                transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "            ])(pil_image).unsqueeze(0).to(interrogator_device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "            del gpu_image\n",
        "            return caption[0]\n",
        "\n",
        "        def load_list(filename):\n",
        "            with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                items = [line.strip() for line in f.readlines()]\n",
        "            return items\n",
        "\n",
        "        def rank(model, image_features, text_array, top_count=1):\n",
        "            top_count = min(top_count, len(text_array))\n",
        "            text_tokens = clip.tokenize([text for text in text_array]).to(interrogator_device)\n",
        "            with torch.no_grad():\n",
        "                text_features = model.encode_text(text_tokens).float()\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            similarity = torch.zeros((1, len(text_array))).to(interrogator_device)\n",
        "            for i in range(image_features.shape[0]):\n",
        "                similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "            similarity /= image_features.shape[0]\n",
        "\n",
        "            top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "            \n",
        "            del similarity\n",
        "\n",
        "            return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "        def interrogate(image, models):\n",
        "\n",
        "            from  torch.cuda.amp import autocast\n",
        "\n",
        "            caption = generate_caption(image)\n",
        "            if len(models) == 0:\n",
        "                print(f\"\\n\\n{caption}\")\n",
        "                return\n",
        "\n",
        "            table = []\n",
        "            bests = [[('',0)]]*5\n",
        "\n",
        "            print('\\n\\033[1mCLIP Interrogator:\\033[0m\\n')\n",
        "\n",
        "            with autocast():\n",
        "\n",
        "                for model_name in models:\n",
        "                    print(f\":magnifying_glass_tilted_right: Interrogating with {model_name}...\")\n",
        "                    model, preprocess = clip.load(model_name)\n",
        "                    model.to(interrogator_device).eval()\n",
        "\n",
        "                    images = preprocess(image).unsqueeze(0).to(interrogator_device)\n",
        "                    with torch.no_grad():\n",
        "                        image_features = model.encode_image(images).float32()\n",
        "                    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                    ranks = [\n",
        "                        rank(model, image_features, mediums),\n",
        "                        rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
        "                        rank(model, image_features, trending_list),\n",
        "                        rank(model, image_features, movements),\n",
        "                        rank(model, image_features, flavors, top_count=3)\n",
        "                    ]\n",
        "\n",
        "                    for i in range(len(ranks)):\n",
        "                        confidence_sum = 0\n",
        "                        for ci in range(len(ranks[i])):\n",
        "                            confidence_sum += ranks[i][ci][1]\n",
        "                        if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "                            bests[i] = ranks[i]\n",
        "\n",
        "                    row = [model_name]\n",
        "                    for r in ranks:\n",
        "                        row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "                    table.append(row)\n",
        "\n",
        "                    clean_env()\n",
        "\n",
        "            display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
        "\n",
        "            flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
        "            medium = bests[0][0][0]\n",
        "            interrogator_prompt = ''\n",
        "            if caption.startswith(medium):\n",
        "                interrogator_prompt = f\"{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
        "            else:\n",
        "                interrogator_prompt = f\"{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
        "\n",
        "            globals().update({'INTERROGATOR_PROMPT': interrogator_prompt})\n",
        "\n",
        "            print('\\033[0m:black_nib: Prompt:')\n",
        "            print(f'{interrogator_prompt}\\033[1m\\n\\n')\n",
        "\n",
        "            del table, image_features, model, preprocess\n",
        "            clean_env()\n",
        "\n",
        "        def do_interrogate(image, show_thumb=False):\n",
        "\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/BLIP')\n",
        "\n",
        "            blip_image_eval_size = 384\n",
        "            blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
        "            blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
        "            blip_model.eval()\n",
        "            blip_model = blip_model.to(interrogator_device)\n",
        "\n",
        "            globals().update({'blip_model': blip_model, 'blip_image_eval_size': blip_image_eval_size})\n",
        "\n",
        "            models = []\n",
        "            if ViTB32: models.append('ViT-B/32')\n",
        "            if ViTB16: models.append('ViT-B/16')\n",
        "            if ViTL14: models.append('ViT-L/14')\n",
        "            if ViTL14_336px: models.append('ViT-L/14@336px')\n",
        "            if RN101: models.append('RN101')\n",
        "            if RN50: models.append('RN50')\n",
        "            if RN50x4: models.append('RN50x4')\n",
        "            if RN50x16: models.append('RN50x16')\n",
        "            if RN50x64: models.append('RN50x64')\n",
        "\n",
        "            if show_thumb:\n",
        "                thumb = image.copy()\n",
        "                thumb.thumbnail([blip_image_eval_size, blip_image_eval_size])\n",
        "                display(thumb)\n",
        "                thumb.close()\n",
        "                del thumb\n",
        "\n",
        "            interrogate(image, models=models)\n",
        "\n",
        "            del blip_model\n",
        "            clean_env()\n",
        "\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "        data_path = f\"{STABLE_DIFFUSION_WORKDIR}/clip-interrogator/data/\"\n",
        "\n",
        "        artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
        "        flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
        "        mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
        "        movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
        "\n",
        "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "        trending_list = [site for site in sites]\n",
        "        trending_list.extend([\"trending on \"+site for site in sites])\n",
        "        trending_list.extend([\"featured on \"+site for site in sites])\n",
        "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "        print(\":check_mark_button: CLIP Interrogator Installed!\")\n",
        "\n",
        "except OSError as e:\n",
        "    raise e\n",
        "except BaseException as e:\n",
        "    raise e\n",
        "finally:\n",
        "    if CLEAR_SETUP_LOG: clear()\n",
        "    print(f\"\\n--[ :confetti_ball::party_popper: \\033[1m\\33[32mEasy Diffusion Environtment Setup Complete\\33[0m :party_popper::confetti_ball: ]--\")\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "import random, pprint\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "# Setup param scraper\n",
        "scraper = paramScraper(settings_template, globals())\n",
        "scraper.scrape('setup')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ucr5_i21xSjv"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"settingsdiffuse\"><font size=\"5\" color=\"default\">**Settings & Diffuse**</font></a>\n",
        " \n",
        "clean_env()\n",
        "init = None # Clear/Set init for next run.\n",
        "\n",
        "SAVE_SETTING_FILE = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### <a name=\"promptsetup\">**Prompt Setup**</a>\n",
        "#@markdown <font size=\"3\">Prompts support [Noodle Soup Prompts](https://github.com/WASasquatch/noodle-soup-prompts/wiki/Terminology-Reference) \\([NSP Prompt Generator](https://rebrand.ly/noodle-soup-prompts)\\)</font>\n",
        "PROMPT = \"A stylish beautiful 3d render portrait of _noun-emote_ cat in a _color_ space helmet on the moon\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`PROMPT_FILE` is a optional text file that contains a prompt ***per*** line. If you use a regular `PROMPT` as well, it will be added as the first prompt in series.</font>\n",
        "NEW_NSP_ON_ITERATION = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to generate NSP once, or on each iteration. Check this if you want each iteration to have a freshly cooked noodle prompt.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"initsetup\">**Init Image Setup**</a>\n",
        "INIT_IMAGE = \"\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`INIT_IMAGE` accepts the following formats</font>\n",
        "#@markdown - <font size=\"3\">A single local, or remote image</font>\n",
        "#@markdown - <font size=\"3\">A `.txt` file containing a single local, or remote image ***per*** line.</font>\n",
        "#@markdown - <font size=\"3\">A path to a local folder containing images.</font>\n",
        "\n",
        "#@markdown <font size=\"3\">**Note:** You can use a prompt file with init images. If you have more images than prompts, it will use the last prompt for all remaining init images.</font> \n",
        "INIT_SCALE = 0.5 #@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">**Note:** Scale of init image from 0 to 1. Lower values adhear more to the image.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"diffusionsettings\">**Diffusion Settings**</a>\n",
        "MODEL_ID = 'CompVis/stable-diffusion-v1-4' #@param [\"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",\"hakurei/waifu-diffusion\"]\n",
        "#@markdown Desired model to diffuse with.\n",
        "REDOWNLOAD_MODEL = False #@param{type: 'boolean'}\n",
        "SAMPLER = 'DEFAULT' #@param [\"DEFAULT\", \"PNDM\", \"LMS\", \"DDIM\"]\n",
        "DDIM_ETA = 0.65 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">`DDIM_ETA` only applies to the DDIM sampler.</font>\n",
        "STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "#@markdown <font size=\"3\">Diffusion steps determines the quality of the final image</font>\n",
        "SEED = 0 #@param {type:'raw'}\n",
        "#@markdown <font size=\"3\">The seed used for the generation. System max value is `9999999999999999`. Leave at `0` for random. You can also enter text encapulated by semi-quotes such as: `'RockyCanyon'`</font>\n",
        "MAX_SEED = 'system_max' #@param{type:'raw'}\n",
        "#@markdown <font size=\"3\">Use `'system_max'` (with semi-quotes) for the maximum system `int`, or define the max random int size (maximum integer length of 16 digits).\n",
        "INCREMENT_ITERATION_SEED = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Increment seed on each iteration.</font>\n",
        "NUM_ITERS = 5 #@param {type:\"slider\", min:1, max:1000, step:1} \n",
        "#@markdown <font size=\"3\">Number of iterations for a given prompt or init image.</font>\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "HEIGHT = 768 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "SCALE = 13.5 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown <font size=\"3\">The CFG `SCALE` determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.</font>\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown <font size=\"3\">If you're using the `LOW_VRAM_PATCH` you <b>must</b> use `autocast`</font><br>\n",
        "IMAGES_FOLDER = \"time_to_stabilize\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">Define a custom folder to saves images within your `images_out` folder. Example: `CAR_CONCEPTS`</font><br>\n",
        "#@markdown <font size=\"3\">**Note:** Path: `/content/Stable_Diffusion/images_out` or with Google Drive `/content/drive/MyDrive/AI/Stable_Diffusion/images_out`</font>\n",
        "\n",
        "#@markdown ---\n",
        "CACHE_PIPELINES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to cache pipes to disk and load on demand (this can speed up diffusion start time)</font>\n",
        "RECACHE_PIPES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">**NOTE:** If you're having trouble loading pipes to start diffusions, check this and run this cell again.</font><br>\n",
        "SKIP_DIFFUSION_RUN = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Skip diffusion run ***If*** `INIT_IMAGE` ***is defined*** to process images.</font>\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">`ENABLE_NSFW_FILTER`: Will return a blurred image for content flagged as NSFW</font>\n",
        "ENABLE_ATTENTION_SLICES = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Enable attention slices to better utilization of available memory at the cost of diffusion speed.</font>\n",
        "LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">You may need this if you're using a GPU with ~16GB VRAM. **Note:** This appplies to non-cached pipes.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"upscalers\">**General Upscaling Settings**</a>\n",
        "#@markdown <font size=\"3\">`IMAGE_UPSCALER`: may not work at resolutions above 512x768/768x512 on GPUs with ~16GB VRAM. Try using ESRGAN in CPU Mode if you're having issues.<br>**Note:** GFPGAN/CodeFormer is good for faces only.</font>\n",
        "IMAGE_UPSCALER = \"None\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"slider\", min:2, max:8, step:2}\n",
        "ESRGAN_MODE = 'CUDA' #@param ['CUDA', 'CPU']\n",
        "#@markdown <font size=\"3\">Real-ESRGAN Device Mode. CUDA is GPU.\n",
        "#@markdown ---\n",
        "#@markdown #### **CodeFormer Upscaling Settings**\n",
        "CODEFORMER_UPSCALE_AMOUNT = 1 #@param {type: 'number'}\n",
        "#@markdown <font size=\"3\">`CF_UPSCALE_AMOUNT` only applies to CodeFormer. Defined the upscale factor for CodeFormer.\n",
        "CODEFORMER_FIDELITY = 0.6 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "#@markdown <font size=\"3\">`CODEFORMER_FIDELITY`: only applies to CodeFormer. Balance the quality (lower number) and fidelity (higher number)</font><br>\n",
        "\n",
        "if CODEFORMER_UPSCALE_AMOUNT <= 0:\n",
        "    CODEFORMER_UPSCALE_AMOUNT = 1\n",
        "else:\n",
        "    CODEFORMER_UPSCALE_AMOUNT = closest_value([1,2,4,8],CODEFORMER_UPSCALE_AMOUNT)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"imageprocessors\">**Image Processor Setup**</a>\n",
        "SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Scale down enhanced images. Useful if you are also using Real-ESRGAN. This will preserve your upscale factor for Real-ESRGAN after GFPGAN or CodeFormer.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"sharpen\">Sharpen Image</a>\n",
        "#@markdown <font size=\"3\">Sharpen the base diffusion image before upscsaling.</font>\n",
        "SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "#@markdown <font size=\"3\">Sharpen iteration amount. `0` for no sharpen.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"kromo\">Kromo Chromatic Aberration</a>\n",
        "CA_DIFFUSE_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Chromatic Aberration to the base diffusion image (pre sharpen if enabled)</font>\n",
        "CA_STRENGTH = 0.1 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration strength</font>\n",
        "CA_JITTER = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration set channel offset pixels</font>\n",
        "CA_OVERLAY = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">Alpha of original image overlay.</font>\n",
        "CA_NO_RADIAL_BLUR = True #@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"median\">Median Filter Image</a>\n",
        "MEDIAN_FILTER_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply a Median Filter effect to the diffusion image. This can be tuned similar to a surface blur for reducing detail.</font>\n",
        "MEDIAN_DIAMETER = 2.5 #@param{type:'slider', min:0.1, max:100, step:0.1}\n",
        "#@markdown <font size=\"3\">Radius of the median filtering effect</font>\n",
        "MEDIAN_SIGMA_COLOR = 75 #@param{type: 'number'}\n",
        "#@markdown <font size=\"3\">Sigma Color filters sigma in the color space. A larger value means that farther colors within the pixel neighborhood will be mixed together, resulting in larger areas of semi-equal color.</font>\n",
        "MEDIAN_SIGMA_SPACE = 75 #@param{type: 'number'}\n",
        "#@markdown <font size=\"3\">Sigma Space silters the sigma in the coordinate space. A larger value means that farther pixels will influence each other as long as their colors are close enough.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"midas\">MiDaS Depth Map</a>\n",
        "EXPORT_MIDAS_DEPTH = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Save a MiDaS depth approximation of the diffusion result</font>\n",
        "MIDAS_TYPE = \"DPT_Large\" #@param [\"DPT_Large\",\"DPT_Hybrid\",\"MiDaS_small\"]\n",
        "#@markdown <font size=\"3\">`MIDAS_TYPE` determines the model to use for depth approximation.</font>\n",
        "MIDAS_MODE = \"CPU\" #@param [\"CPU\",\"CUDA\"]\n",
        "#@markdown <font size=\"3\">**CPU Mode:** If you get: \"`RuntimeError: \"linspace_cpu\" not implemented for 'Half'`\" something has changed with CPU and you need to disconnect/reconnect (Google Colab)</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"fdof\">Fake Depth of Field Filter</a>\n",
        "FDOF_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Fake Depth of Field to the image based on the MiDaS Depth Map</font>\n",
        "FDOF_REPLACE_IMAGE = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Replace original diffusion image saved to disk with FDOF image. This will also pipe the image through for upscalers.</font>\n",
        "FDOF_RADIUS = 8.5 #@param {type:'slider', min: 0.1, max:100, step:0.1}\n",
        "#@markdown <font size=\"3\">Depth of Field Blur Radius\n",
        "FDOF_SAMPLES = 1 #@param{type:'slider', min:1, max:5, step:1}\n",
        "#@markdown <font size=\"3\">Resample the image with DOF (can create a stronger DOF effect based on the MiDas Depth Map)</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"tileable\">Tilable Seamless Image</a>\n",
        "TILEABLE_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Process the image as a seamless tileable texture.</font>\n",
        "TILED = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Tile the image 2x2 (Ex a 512x512 image would be tiled to 1024x1024)</font>\n",
        "TILE_OVERLAP = 0.1 #@param{type:'slider', min:0.01, max:1.0, step:0.01}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"clipinterrogator\">CLIP Interrogator</a>\n",
        "#@markdown <font size=\"3\">CLIP Interrogator requires a substantial chunk of GPU VRAM, and may not be suited for low VRAM cards while running diffusions. Use at your own risk.<br />Using this for init_images with `SKIP_DIFFUSION_RUN` enabled allows you to batch interrogate images to scrape prompt ideas.</font>\n",
        "INTERROGATE_INIT_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Interrogate `INIT_IMAGE`</font>\n",
        "INTERROGATE_DIFFUSION_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Interrogate diffusion result after generation.</font>\n",
        "\n",
        "#@markdown **Interrogator CLIP Models**\n",
        "ViTB32 = False #@param{type:\"boolean\"}\n",
        "ViTB16 = False #@param{type:\"boolean\"}\n",
        "ViTL14 = True #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = False #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">These models are only relevant if you're interrogating init images or diffusion results with CLIP Interrogator.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"othersettings\">**Other Diffusion Settings**</a>\n",
        "IMAGES_DISPLAY_ABOVE_LOG = False #@param{type: 'boolean'}\n",
        "#@markdown Display organized JS Images above log output, not below.</font>\n",
        "USE_BASIC_IMAGE_DISPLAY = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Use basic image output instead of organized JS Image Output</font>\n",
        "CLEAR_LOG_BETWEEN_ITERATIONS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Clear the output log between each iteration. (Organized JS Mode Only)</font>\n",
        "\n",
        "download_model(MODEL_ID, REDOWNLOAD_MODEL)\n",
        "\n",
        "ESRGAN_MODE = ESRGAN_MODE.lower()\n",
        "\n",
        "if LOW_VRAM_PATCH and PRECISION is not 'autocast': \n",
        "    print(f\"PRECISION must be 'autocast' when running in low vram compatibility mode! Defaulting to autocast...\")\n",
        "    PRECISION = 'autocast'\n",
        "\n",
        "precision_scope = autocast if PRECISION is 'autocast' else nullcontext\n",
        "\n",
        "# Max Seed and Custom Seed Setup\n",
        "if MAX_SEED is 'system_max':\n",
        "    MAX_SEED = 9999999999999999\n",
        "else:\n",
        "    MAX_SEED = int(MAX_SEED)\n",
        "\n",
        "text_seed = None\n",
        "if type(SEED) is str:\n",
        "    text_seed = SEED\n",
        "    SEED = text2seed(SEED, 16)\n",
        "else:\n",
        "    SEED = int(SEED)\n",
        "\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "GDRIVE_OUT_PATH = f'{GDRIVE_WORKDIR}/images_out/{IMAGES_FOLDER}'\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "    if not os.path.exists(GDRIVE_OUT_PATH):\n",
        "        os.makedirs(GDRIVE_OUT_PATH)\n",
        "    OUTDIR = GDRIVE_OUT_PATH        \n",
        "\n",
        "# JavaScript Compatible Boolean\n",
        "if IMAGES_DISPLAY_ABOVE_LOG:\n",
        "    IMAGES_DISPLAY_ABOVE_LOG = 1\n",
        "else:\n",
        "    IMAGES_DISPLAY_ABOVE_LOG = 0\n",
        "\n",
        "print(f\"Images Output Directory: {OUTDIR}\\n\")\n",
        "\n",
        "# Check Upscaling Mode\n",
        "if IMAGE_UPSCALER == 'GFPGAN' and not INSTALL_GFPGAN:\n",
        "    print(\":WARNING: GFPGAN Face Restoration is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'Enhanced Real-ESRGAN' and not INSTALL_ESRGAN:\n",
        "    print(\":WARNING: Real-ESRGAN is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'CodeFormer' and not INSTALL_CODEFORMER:\n",
        "    print(\":WARNING: CodeFormer is not installed! Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'GFPGAN + Enhanced ESRGAN':\n",
        "    if not INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: GFPGAN is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_GFPGAN:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to GFPGAN...\")\n",
        "        IMAGE_UPSCALER = 'GFPGAN'\n",
        "if IMAGE_UPSCALER == 'CodeFormer + Enhanced ESRGAN':\n",
        "    if not INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: CodeFormer is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_CODEFORMER:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to CodeFormer...\")\n",
        "        IMAGE_UPSCALER = 'CodeFormer'\n",
        "\n",
        "# Nearest value to UPSCALE_AMOUNT\n",
        "nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "\n",
        "scraper.updateGlobals(globals())\n",
        "scraper.scrape('upscalers')\n",
        "scraper.scrape('image_processing')\n",
        "scraper.scrape('other_settings')\n",
        "\n",
        "# Setup Piping Cache\n",
        "if CACHE_PIPELINES:\n",
        "    pipe_cache = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "    if not os.path.exists(model_cache):\n",
        "        os.makedirs(model_cache)\n",
        "    if not os.path.exists(pipe_cache):\n",
        "        os.makedirs(pipe_cache)\n",
        "    if ( not os.path.exists(f'{pipe_cache}/LOW_VRAM_PIPE.obj') \n",
        "         or not os.path.exists(f'{pipe_cache}/IMG2IMG_PIPE.obj') \n",
        "         or not os.path.exists(f'{pipe_cache}/DEFAULT_PIPE.obj') ):\n",
        "        print('\\n:gear: Setting up Stable Diffusion Pipeline Cache...')\n",
        "        cache_pipes(MODEL_ID, model_cache, pipe_cache)\n",
        "\n",
        "# Diffuse Function\n",
        "def diffuse_run():\n",
        "\n",
        "    clean_env()\n",
        "\n",
        "    global SEED, UPSCALE_AMOUNT, EXPORT_MIDAS_DEPTH, FDOF_IMAGE\n",
        "    if not CACHE_PIPELINES: global pipe\n",
        "\n",
        "    if ORIG_SEED is 0 and SEED is 0:\n",
        "        SEED = random.randint(0,MAX_SEED)\n",
        "    else:\n",
        "        if INCREMENT_ITERATION_SEED and iteration > 0:\n",
        "            SEED += 1\n",
        "\n",
        "    if not os.path.exists(OUTDIR):\n",
        "        os.makedirs(OUTDIR)\n",
        "\n",
        "    scraper.updateGlobals(globals())\n",
        "    scraper.scrape('prompts')\n",
        "    scraper.scrape('inits')\n",
        "    scraper.scrape('diffusion_settings')\n",
        "\n",
        "    epoch_time = int(time.time())\n",
        "    if not SKIP_DIFFUSION_RUN:\n",
        "        encoded_seed = f' (Encoded from: {text_seed})' if text_seed else ''\n",
        "        gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "        eta_prev = f' (ETA: {DDIM_ETA})' if SAMPLER is 'DDIM' else ''\n",
        "        print(f\"\\n\\033[1mBatch {(i+1)}/{len(ITERATE_THIS)} Iteration {(iteration+1)}/{NUM_ITERS}\\033[0m\")\n",
        "        print(f':seedling: Seed: \\033[1m{SEED}{encoded_seed}\\033[0m, :triangular_ruler: Scale: \\033[1m{SCALE}\\033[0m, :footprints: Steps: \\033[1m{STEPS}\\033[0m, :artist_palette: Sampler: {SAMPLER}{eta_prev} :framed_picture: Resolution: \\033[1m{WIDTH}x{HEIGHT}')\n",
        "        midas_prev = f' (Type: \\033[1m{MIDAS_TYPE}\\033[0m, Mode: \\033[1m{MIDAS_MODE}\\033[0m)' if EXPORT_MIDAS_DEPTH else ''\n",
        "        ca_prev = f' (Strength: \\033[1m{CA_STRENGTH}\\033[0m, Jitter: \\033[1m{CA_JITTER}\\033[0m, Overlay: \\033[1m{CA_OVERLAY}\\033[0m, No Radial Blur: \\033[1m{CA_NO_RADIAL_BLUR}\\033[0m)\\n' if CA_DIFFUSE_IMAGE else ''\n",
        "        print(f'Scale Down: \\033[1m{SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN}\\033[0m, Sharpen Passes: \\033[1m{SHARPEN_AMOUNT}\\033[0m, Chromatic Aberration: \\033[1m{CA_DIFFUSE_IMAGE}\\033[0m{ca_prev}Depth Export: \\033[1m{EXPORT_MIDAS_DEPTH}\\033[0m{midas_prev}\\n')\n",
        "        print(\"\\033[0m:black_nib: Prompt:\\033[1m\")\n",
        "        printPrompt(PROMPT)\n",
        "        print(\"\\033[0m\\n\")\n",
        "\n",
        "    if init is not None:\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print(\"Resized Init Image:\")\n",
        "            display(original_init)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Resized Init Image B: {(i+1)} I: {(iteration+1)}', original_init)\n",
        "        if SKIP_DIFFUSION_RUN:\n",
        "            image = original_init.copy()\n",
        "        if INTERROGATE_INIT_IMAGE:\n",
        "            do_interrogate(image)\n",
        "            scraper.updateGlobals(globals())\n",
        "            scraper.scrape('clip_interrogator')\n",
        "            \n",
        "\n",
        "    # Load Cached Pipelines\n",
        "    if CACHE_PIPELINES and not SKIP_DIFFUSION_RUN:\n",
        "        clean_env()\n",
        "        stt = int(time.time())\n",
        "        print(':gear: Loading Stable Diffusion Pipeline from cache...')\n",
        "        if init is None and LOW_VRAM_PATCH:\n",
        "            pipe = joblib.load(f'{pipe_cache}/LOW_VRAM_PIPE.obj')\n",
        "            del pipe.vae.encoder\n",
        "            pipe.model_id = MODEL_ID\n",
        "            clean_env()\n",
        "        elif init is not None:\n",
        "            pipe = joblib.load(f'{pipe_cache}/IMG2IMG_PIPE.obj')\n",
        "            pipe.model_id = MODEL_ID\n",
        "            clean_env()\n",
        "        else:\n",
        "            pipe = joblib.load(f'{pipe_cache}/DEFAULT_PIPE.obj')\n",
        "            pipe.model_id = MODEL_ID\n",
        "            clean_env()\n",
        "        if ENABLE_ATTENTION_SLICES:\n",
        "            print(':gear: Attention Slices Enabled.')\n",
        "            pipe.enable_attention_slicing()\n",
        "            #optimize_attention(pipe.unet)\n",
        "        else:\n",
        "            pipe.disable_attention_slicing()\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f':check_mark_button: Pipeline loaded in {fnt}')\n",
        "\n",
        "    # Setup Pipes\n",
        "    if not SKIP_DIFFUSION_RUN:\n",
        "        if SAMPLER is 'DEFAULT':\n",
        "            pipe.scheduler = PNDMScheduler (\n",
        "                trained_betas= None,\n",
        "                beta_end= 0.012,\n",
        "                beta_schedule= \"scaled_linear\",\n",
        "                beta_start= 0.00085,\n",
        "                num_train_timesteps= 1000,\n",
        "                skip_prk_steps= True\n",
        "            )\n",
        "        if SAMPLER == 'PNDM':\n",
        "            pipe.scheduler = PNDMScheduler(\n",
        "                trained_betas= None,\n",
        "                beta_start=0.00085, \n",
        "                beta_end=0.012, \n",
        "                beta_schedule=\"scaled_linear\", \n",
        "                num_train_timesteps=1000\n",
        "            )\n",
        "        elif SAMPLER == 'LMS':\n",
        "            pipe.scheduler = LMSDiscreteScheduler(\n",
        "                trained_betas= None,\n",
        "                beta_start=0.00085,\n",
        "                beta_end=0.012,\n",
        "                beta_schedule=\"scaled_linear\",\n",
        "                num_train_timesteps=1000\n",
        "            )\n",
        "        elif SAMPLER == 'DDIM':\n",
        "            pipe.scheduler = DDIMScheduler(\n",
        "                trained_betas= None,\n",
        "                beta_start=0.00085,\n",
        "                beta_end=0.012,\n",
        "                beta_schedule=\"scaled_linear\", \n",
        "                clip_sample=False,\n",
        "                set_alpha_to_one=False\n",
        "            )\n",
        "\n",
        "        # Do diffusion\n",
        "        pipeout = None\n",
        "        try:\n",
        "            stt = int(time.time())\n",
        "            print(f\":alembic: Starting Diffusion run with {MODEL_ID}\")\n",
        "            if init is not None:\n",
        "                with autocast(\"cuda\"):\n",
        "                    pipeout = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init, strength=INIT_SCALE, guidance_scale=SCALE, generator=gen_seed)\n",
        "                    image = pipeout[\"sample\"][0]\n",
        "            else:\n",
        "                if SAMPLER == 'ddim':\n",
        "                    pipeout = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)\n",
        "                    image = pipeout[\"sample\"][0]\n",
        "                else:\n",
        "                    pipeout = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)\n",
        "                    image = pipeout[\"sample\"][0]\n",
        "        except BaseException as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            if pipeout and pipeout['nsfw_content_detected'][0] and ENABLE_NSFW_FILTER:\n",
        "                image = image.filter(ImageFilter.GaussianBlur(radius = 18))\n",
        "            if CACHE_PIPELINES and not SKIP_DIFFUSION_RUN:\n",
        "                del pipeout, pipe\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f':check_mark_button: Diffusion completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "\n",
        "    if INTERROGATE_DIFFUSION_IMAGE:\n",
        "        clean_env()\n",
        "        interrogate_image = image.copy()\n",
        "\n",
        "    # Doing FDoF? We need depth output\n",
        "    if FDOF_IMAGE and not EXPORT_MIDAS_DEPTH:\n",
        "        if INSTALL_MIDAS:\n",
        "            EXPORT_MIDAS_DEPTH = True\n",
        "            print(':WARNING: Enabling \\'EXPORT_MIDAS_DEPTH\\' (True) for Depth Approximation necessary for Fake Depth of Field.')\n",
        "        else:\n",
        "            print(':WARNING: Unable to generate Fake Depth of Field! MiDaS Compatibility is not installed! Please enable \\'INSTALL_MIDAS\\' and re-run the setup cell.')\n",
        "            EXPORT_MIDAS_DEPTH = False\n",
        "            FDOF_IMAGE = False\n",
        "\n",
        "    # Do Depth Export\n",
        "    depth_image = None\n",
        "    if INSTALL_MIDAS:\n",
        "        if EXPORT_MIDAS_DEPTH:\n",
        "            stt = int(time.time())\n",
        "            print(\"Approximating diffusion depth...\")\n",
        "            midas = torch.hub.load(\"intel-isl/MiDaS\", MIDAS_TYPE)\n",
        "            device = torch.device(\"cuda\") if torch.cuda.is_available() and MIDAS_MODE is 'CUDA' else torch.device(\"cpu\")\n",
        "\n",
        "            midas.to(device).eval()\n",
        "            midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "            if MIDAS_TYPE == \"DPT_Large\" or MIDAS_TYPE == \"DPT_Hybrid\":\n",
        "                transform = midas_transforms.dpt_transform\n",
        "            else:\n",
        "                transform = midas_transforms.small_transform\n",
        "\n",
        "            import cv2\n",
        "            img = cv2.imread(filedir)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            input_batch = transform(img).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                prediction = midas(input_batch)\n",
        "\n",
        "                prediction = torch.nn.functional.interpolate(\n",
        "                    prediction.unsqueeze(1),\n",
        "                    size=img.shape[:2],\n",
        "                    mode=\"bicubic\",\n",
        "                    align_corners=False,\n",
        "                ).squeeze()\n",
        "\n",
        "            depth = prediction.cpu().numpy()\n",
        "            depth = (depth * 255 / (np.max(depth)+1)).astype('uint8')\n",
        "            depth_image = Image.fromarray(depth)\n",
        "            depth_image.save(filedir.replace('.png', '_depth.png'))\n",
        "            del midas, device, midas_transforms\n",
        "            del transform, img, input_batch, prediction, depth\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Depth approximation completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    original_displayed = False\n",
        "    # Do Median Filter\n",
        "    if MEDIAN_FILTER_IMAGE:\n",
        "        original_displayed = True\n",
        "        stt = int(time.time())\n",
        "        print('Applying Median Filter...')\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print(\"Original Diffusion Image:\")\n",
        "            display(image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Original Diffusion B: {(i+1)} I: {(iteration+1)}', image)\n",
        "        image = medianFilter(image, MEDIAN_DIAMETER, MEDIAN_SIGMA_COLOR, MEDIAN_SIGMA_SPACE)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Applied Median Filter in {fnt}')\n",
        "\n",
        "    # Do Chromatic Aberration\n",
        "    if INSTALL_KROMO:\n",
        "        if CA_DIFFUSE_IMAGE:\n",
        "            stt = int(time.time())\n",
        "            res = ''\n",
        "            print(f\"Applying chromatic aberration to result image.\\n\")\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "            ca_no_blur = '-n ' if CA_NO_RADIAL_BLUR else ''\n",
        "            res += subprocess.run(f'python kromo.py -s {CA_STRENGTH} -j {CA_JITTER} -y {CA_OVERLAY} {ca_no_blur}-o {filedir} {filedir}'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if res.strip() != '':\n",
        "                print(res)\n",
        "            image = Image.open(filedir).resize((WIDTH,HEIGHT))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            fnt = time_format(int(time.time() - stt))\n",
        "            print(f'Chromatic aberration applied in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    # Do Sharpen\n",
        "    if SHARPEN_AMOUNT > 0:\n",
        "        stt = int(time.time())\n",
        "        print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\")\n",
        "        image = sharpenImage(image, SHARPEN_AMOUNT)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Sharpening completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    # Do FDOF\n",
        "    fdof_title = None\n",
        "    if FDOF_IMAGE:\n",
        "        if depth_image:\n",
        "            stt = int(time.time())\n",
        "            print('Applying Fake Depth of Field...')\n",
        "            fdof_image = portraitBlur(image, depth_image, FDOF_RADIUS, FDOF_SAMPLES).convert('RGB')\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Applied FDOF in {fnt}')\n",
        "            if FDOF_REPLACE_IMAGE:\n",
        "                fdof_title = 'FDOF Image'\n",
        "                if not original_displayed:\n",
        "                    if USE_BASIC_IMAGE_DISPLAY:\n",
        "                        print('Original Diffusion Image:')\n",
        "                        display(image)\n",
        "                    else:\n",
        "                        displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Original Diffusion B: {(i+1)} I: {(iteration+1)}', image)\n",
        "                fdof_image.save(filedir)\n",
        "                image = Image.open(filedir)\n",
        "                fdof_image.close()\n",
        "                del fdof_image \n",
        "        else:\n",
        "            print(f':WARNING: Warning: depth_image is not generated! Is MiDaS compatibility installed?')\n",
        "\n",
        "    # Do Seamless Image\n",
        "    if TILEABLE_IMAGE:\n",
        "        stt = int(time.time())\n",
        "        print(f'Processing tiled seamless image...')\n",
        "        image = image.resize((WIDTH, HEIGHT))\n",
        "        image.save(f'{OUTDIR}/tile_temp.png')\n",
        "        tileFilename = filename.replace('.png','_seamless.png')\n",
        "        seamlessPath = os.path.join(OUTDIR, tileFilename)\n",
        "        tileFlag = f' --tile' if TILED else ''\n",
        "        if EXPORT_MIDAS_DEPTH and depth_image:\n",
        "            depth_seamlessPath = os.path.join(OUTDIR, filename.replace('.png','_depth_seamless.png'))\n",
        "            depthFileDir = filedir.replace('.png', '_depth.png')\n",
        "            res = subprocess.run(f'img2texture {depthFileDir} {depth_seamlessPath} --overlap {TILE_OVERLAP}{tileFlag}'.split(' '), capture_output=True, text=True, input=\"y\")\n",
        "        print(f'Saving tiled image to: {seamlessPath}')\n",
        "        res = subprocess.run(f'img2texture {OUTDIR}/tile_temp.png {seamlessPath} --overlap {TILE_OVERLAP}{tileFlag}'.split(' '), capture_output=True, text=True, input=\"y\")\n",
        "        seamless_image = Image.open(seamlessPath).convert('RGB')\n",
        "        tiled_image = Image.open(seamlessPath.replace('.png','_2x2.jpg')).convert('RGB')\n",
        "        os.remove(f'{OUTDIR}/tile_temp.png')\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Processed tiled seamless image in {fnt}')\n",
        "\n",
        "    main_img_title = 'SD Image'\n",
        "    if SKIP_DIFFUSION_RUN or original_displayed:\n",
        "        main_img_title = 'Processed Image'\n",
        "    elif fdof_title:\n",
        "        main_img_title = fdof_title\n",
        "\n",
        "    # Display Diffusion Image\n",
        "    if USE_BASIC_IMAGE_DISPLAY:\n",
        "        print(\"Diffusion Image:\")\n",
        "        display(image)\n",
        "    else:\n",
        "        displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'{main_img_title} B: {(i+1)} I: {(iteration+1)}', image)\n",
        "\n",
        "    # Display Depth Image\n",
        "    try:\n",
        "        if depth_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Depth Map:')\n",
        "                display(depth_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Depth Map B: {(i+1)} I: {(iteration+1)}', depth_image)\n",
        "            depth_image.close()\n",
        "            del depth_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    # Display FDOF Image\n",
        "    try:\n",
        "        if fdof_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('FDOF Image:')\n",
        "                display(fdof_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'FDOF B: {(i+1)} I: {(iteration+1)}', fdof_image)\n",
        "            fdof_image.close()\n",
        "            del fdof_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    # Display Tiled Seamless Image\n",
        "    try:\n",
        "        if seamless_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Seamless Image:')\n",
        "                display(seamless_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Seamless Image B: {(i+1)} I: {(iteration+1)}', seamless_image)\n",
        "            seamless_image.close()\n",
        "            del seamless_image\n",
        "    except NameError:\n",
        "        pass\n",
        "    try:\n",
        "        if tiled_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Tiled Image:')\n",
        "                display(tiled_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Tiled Image B: {(i+1)} I: {(iteration+1)}', tiled_image)\n",
        "            tiled_image.close()\n",
        "            del tiled_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    if 'ESRGAN' in IMAGE_UPSCALER:\n",
        "        os.chdir(f\"{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN\")\n",
        "        if not os.path.exists(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "    if INSTALL_GFPGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('GFPGAN Image:')\n",
        "                display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'GFPGAN B: {(i+1)} I: {(iteration+1)}', Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(f'Moving enhanced image to {OUTDIR}')\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'GFPGAN Face Restoration completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            print(':multiply: Real-ESRGAN Upscaling... ')\n",
        "            print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            UPSCALE_AMOUNT = nearest_value\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            sr_image = upscale(image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Real-ESRGAN Image:')\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "            try:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            except NameError:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            del sr_image\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Enhanced Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            # GFPGAN\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s 1 --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('GFPGAN Image:')\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'GFPGAN B: {(i+1)} I: {(iteration+1)}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Real-ESRGAN Image:')\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            del sr_image\n",
        "            enhanced_image.close()\n",
        "            del enhanced_image\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'GFPGAN + Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "            stt = int(time.time())\n",
        "            fidelity = float(CODEFORMER_FIDELITY)\n",
        "            if fidelity is 0:\n",
        "                fidelity = '0.0'\n",
        "            clean_env()\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            # It was behaving weird, hence why I am doing this the weird way\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {fidelity} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --upscale {CODEFORMER_UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{fidelity}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('CodeFormer Image:')\n",
        "                display(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png'))\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'CodeFormer B: {(i+1)} I: {(iteration+1)}', Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png'))\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'CodeFormer Face Restoration completed in {fnt}')\n",
        "            clean_env()\n",
        "    else:\n",
        "        print(\"CodeFormer is not installed! Please check CodeFormer and run the environment setup again.\")\n",
        "\n",
        "\n",
        "    if INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            fidelity = float(CODEFORMER_FIDELITY)\n",
        "            if fidelity is 0:\n",
        "                fidelity = '0.0'\n",
        "            clean_env()\n",
        "            # CodeFormer\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {fidelity} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{fidelity}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('CodeFormer Image:')\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'CodeFormer B: {(i+1)} I: {(iteration+1)}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Real-ESRGAN Image:')\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            del sr_image\n",
        "            enhanced_image.close()\n",
        "            del enhanced_image\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'CodeFormer + Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "   \n",
        "    if INTERROGATE_DIFFUSION_IMAGE and not SKIP_DIFFUSION_RUN:\n",
        "        clean_env()\n",
        "        do_interrogate(interrogate_image.copy())\n",
        "        interrogate_image.close()\n",
        "        del interrogate_image\n",
        "        scraper.updateGlobals(globals())\n",
        "        scraper.scrape('clip_interrogator')\n",
        "\n",
        "    if SAVE_SETTING_FILE:\n",
        "        epoch_time = int(time.time())\n",
        "        with open(f'{OUTDIR}/{epoch_time}_settings.txt', 'w') as file:\n",
        "            file.write(json.dumps(scraper.params, indent=4))\n",
        "\n",
        "    image.close()\n",
        "    del image\n",
        "\n",
        "    if CLEAR_LOG_BETWEEN_ITERATIONS and not USE_BASIC_IMAGE_DISPLAY:\n",
        "        clearOutputArea(i, iteration);\n",
        "\n",
        "# End Diffuse Function\n",
        "\n",
        "if SKIP_DIFFUSION_RUN:\n",
        "    print(f'Skipping Diffusion Run to Process Images...')\n",
        "    PROMPT = 'None'\n",
        "    PROMPT_FILE = ''\n",
        "    NUM_ITERS = 1\n",
        "else:\n",
        "    # Setup Prompts\n",
        "    if PROMPT.lower() in [None, '', 'none'] and PROMPT_FILE in [None, '', 'none']:\n",
        "        raise Exception(\"PROMPT and PROMPT_FILE are empty! You need to provide a PROMPT or PROMPT_FILE!\")\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "        \n",
        "# Insert prompt string first\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "#Get corrected sizes\n",
        "WX = (WIDTH//64)*64;\n",
        "HY = (HEIGHT//64)*64;\n",
        "if int(WX) != int(WIDTH) or int(HY) != int(HEIGHT):\n",
        "    print(f'Changing output size to {WX}x{HY}. Dimensions must by multiples of 64.')\n",
        "    WIDTH = WX\n",
        "    HEIGHT = HY\n",
        "\n",
        "# Setup init_iamge\n",
        "inits = None\n",
        "last_pipe_type = None\n",
        "if INIT_IMAGE.lower() not in [None, '', 'none']:\n",
        "    if INIT_IMAGE.lower().startswith('http://') or INIT_IMAGE.lower().startswith('https://'):\n",
        "        inits = INIT_IMAGE\n",
        "    else:\n",
        "        inits = getInitImages(INIT_IMAGE, True)\n",
        "    if inits is not None:\n",
        "        pipe_type = 'img2img'\n",
        "    else:\n",
        "        print(f\":WARNING: No valid image(s) found in {INIT_IMAGE}. Switching to default Text-to-Image run...\")\n",
        "        pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "else:\n",
        "    pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "\n",
        "# Check if there are images before skipping diffusion\n",
        "if SKIP_DIFFUSION_RUN and inits is None:\n",
        "    SystemExit(\"INIT_IMAGE must be defined to with valid image(s) to skip diffusion run!\")\n",
        "\n",
        "# Initiate non-cached pipelines\n",
        "if not CACHE_PIPELINES and not SKIP_DIFFUSION_RUN:\n",
        "    print(\"Setting up diffusion model pipeline...\")\n",
        "    try:\n",
        "        if pipe:\n",
        "            print(\"Pipeline already in memory. Starting diffusion environment...\\n\")\n",
        "    except NameError:\n",
        "        pipe = setup_pipes(pipe_type, MODEL_ID, model_cache)\n",
        "        pass\n",
        "    if pipe_type is not last_pipe_type:\n",
        "        pipe = setup_pipes(pipe_type, MODEL_ID, model_cache)\n",
        "    if ENABLE_ATTENTION_SLICES:\n",
        "        print(':gear: Attention Slices Enabled.')\n",
        "        pipe.enable_attention_slicing()\n",
        "        #optimize_attention(pipe.unet)\n",
        "    else:\n",
        "        pipe.disable_attention_slicing()\n",
        "        print(':check_mark_button: Pipeline setup complete.')\n",
        "\n",
        "last_pipe_type = pipe_type\n",
        "\n",
        "with torch.no_grad():\n",
        "    with precision_scope(\"cuda\"):\n",
        "\n",
        "        # Hack in Image List Support\n",
        "        DO = None\n",
        "        if type(inits) is list:\n",
        "            ITERATE_THIS = inits\n",
        "            DO = 'inits'\n",
        "        else:\n",
        "            ITERATE_THIS = PROMPTS\n",
        "            DO = 'prompts'\n",
        "\n",
        "        i = 0\n",
        "        for pi in ITERATE_THIS: # Replace PROMPTS with ITERATE_THIS switch\n",
        "\n",
        "            if DO is 'inits':\n",
        "                init = pi\n",
        "                if i > len(PROMPTS)-1:\n",
        "                    pi = PROMPTS[-1]\n",
        "                else:\n",
        "                    pi = PROMPTS[i]\n",
        "            elif DO is 'prompts':\n",
        "                if inits is not None:\n",
        "                    init = inits\n",
        "            if init:    \n",
        "                from PIL import ImageOps\n",
        "                init = Image.open(fetch(init)).convert(\"RGB\")\n",
        "                init = ImageOps.exif_transpose(init)\n",
        "                init = init.resize((WIDTH,HEIGHT))\n",
        "                original_init = init\n",
        "                init = preprocess(init)\n",
        "\n",
        "            # Define Run Prompt\n",
        "            if NEW_NSP_ON_ITERATION is not True:\n",
        "                PROMPT = nsp_parse(pi)\n",
        "\n",
        "            for iteration in range(NUM_ITERS):\n",
        "\n",
        "                # Define Iteration Prompt\n",
        "                if NEW_NSP_ON_ITERATION:\n",
        "                    PROMPT = nsp_parse(pi)\n",
        "\n",
        "                try:\n",
        "\n",
        "                    diffuse_run()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if 'out of memory' in str(e):\n",
        "                        print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! If this error persists, the GPU may have crashed, and requires a disconnect/re-run.\")\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise e\n",
        "                except KeyboardInterrupt as e:\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except Exception as e:\n",
        "                    raise e\n",
        "                finally:\n",
        "                    clean_env()\n",
        "                    \n",
        "            i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RjZhn2vwEydT"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"cleanenv\"><font color=\"orange\">**Clean Environment Up**</font></a>\n",
        "#@markdown <font size=\"3\">**Soft Reset** the environment by deleting pipes, models, and image handlers from memory.<br><br>\n",
        "#@markdown **Note:** Before using this cell, give a minute for the system itself to flush some stuff. This will give a higher chance of this function working.<br>\n",
        "#@markdown **Note 2:** Sometimes you'll get a persistent OOM bug when the GPU has been unallocated from your session. This is common with the new (09/2022) Free Colab Sessions</font>\n",
        "\n",
        "try:\n",
        "    del pipe; del midas; del transform; del prediction; del input_batch; del depth; del depth_image; del image; del sr_image; del enhanced_image; del img; del init; del original_init\n",
        "except NameError:\n",
        "    pass\n",
        "finally:\n",
        "    clean_env(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOY91EzZ1nQH"
      },
      "source": [
        "## <font color=\"default\">**Menu**</font>\n",
        "- <a href=\"#changelog\">**Change Log**</a>\n",
        "- <a href=\"#gpustatus\">**Check GPU Status**</a>\n",
        "- #### <a href=\"#setupenv\">**Setup Environment**</a>\n",
        " - <a href=\"#googledrive\">**Google Drive Options**</a>\n",
        " - <a href=\"#optionalfeats\">**Install Optional Features**</a>\n",
        " - <a href=\"#otherinstall\">**Other Install Options**</a>\n",
        "- #### <a href=\"#settingsdiffuse\">**Settings & Diffuse**</a>\n",
        " - <a href=\"#promptsetup\">**Prompt Setup**</a>\n",
        " - <a href=\"#initsetup\">**Init Image Setup**</a>\n",
        " - <a href=\"#diffusionsettings\">**Diffusion Settings**</a>\n",
        " - <a href=\"#upscalers\">**Upscaling Setup**</a>\n",
        " - <a href=\"#imageprocessors\">**Image Processing Setup**</a>\n",
        "   - <a href=\"#sharpen\">**Sharpen Image**</a>\n",
        "   - <a href=\"#kromo\">**Kromo Chromatic Aberration**</a>\n",
        "   - <a href=\"#median\">**Median Filter Image**</a>\n",
        "   - <a href=\"#midas\">**MiDaS Depth Export**</a>\n",
        "   - <a href=\"#fdof\">**Fake Depth of Field Filter**</a>\n",
        "   - <a href=\"#tileable\">**Tileable Seamless Image**</a>\n",
        " - <a href=\"#clipinterrogator\">**CLIP Interrogator**</a>\n",
        " - <a href=\"#othersettings\">**Other Diffusion Settings**</a>\n",
        "- <a href=\"#cleanenv\">**Clean Environment Up**</a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "02a2872ef89789832e0a654d6c95a175dab3d7e4133113b4cef309e372e0ba06"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
