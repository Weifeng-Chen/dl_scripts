{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(image_features, text_features, labels, logit_scale):\n",
    "    # 计算相似度，支持多个样本的情况（比如一个图片有多个caption）\n",
    "    # img2txt计算的时候要用到，因为一张图片可能对应多个文本。\n",
    "    # txt2img计算的时候不需要（一般一个text只有一个对应图片）\n",
    "    metrics = {}\n",
    "    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()\n",
    "    logits_per_text = logits_per_image.t().detach().cpu()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "\n",
    "    label2idx = {}  # 计算label到idx的映射。\n",
    "    repeat_id = []\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in label2idx:\n",
    "            label2idx[label] = [i]\n",
    "        else:\n",
    "            # 表示该index的标签出现过，记录这个index，后续算txt2img分数的时候，这些index的权值要降低。\n",
    "            label2idx[label].append(i)\n",
    "            repeat_id.append(i)\n",
    "    # print(label2idx)    # 标注了每个label的idx\n",
    "\n",
    "    # print('repeat_id:', repeat_id)\n",
    "    ground_truth = [label2idx[label] for label in labels]\n",
    "    # print(ground_truth)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        # print(name, logit.shape)\n",
    "        if name == 'text_to_image':\n",
    "            logit[:, repeat_id] -= 1e8   # 这部分的分数要降低。（重复出现的图片，直接忽略）\n",
    "        r1_stat, r5_stat, r10_stat = [], [], []\n",
    "        ranking = torch.argsort(logit, descending=True) # index of the largest element to the smallest\n",
    "        # print(name, ranking[:, :10])\n",
    "        for i, each_query in enumerate(ranking[:, :10]):\n",
    "            for j, q in enumerate(each_query):\n",
    "                if q in ground_truth[i]:\n",
    "                    if j == 0:\n",
    "                        r1_stat.append(1)\n",
    "                        r5_stat.append(1)\n",
    "                        r10_stat.append(1)\n",
    "                        break\n",
    "                    if j < 5:\n",
    "                        r5_stat.append(1)\n",
    "                        r10_stat.append(1)\n",
    "                        break\n",
    "                    if j < 10:\n",
    "                        r10_stat.append(1)\n",
    "                        break\n",
    "        print(f'{name} r1:{sum(r1_stat)/len(logit)}, r5:{sum(r5_stat)/len(logit)}, r10:{sum(r10_stat)/len(logit)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO-CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "class COCO_CN(Dataset):\n",
    "    def __init__(self, img_root_path='/home/chenweifeng/dataset/coco', \\\n",
    "                test_img_path='/home/chenweifeng/dataset/coco/coco-cn-version1805v1.1/coco-cn_test.txt', \\\n",
    "                annot_path = '/home/chenweifeng/dataset/coco/coco-cn-version1805v1.1/imageid.human-written-caption.txt', \\\n",
    "                transform=None):\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self.labels = []\n",
    "        self.root = img_root_path\n",
    "        \n",
    "        test_path = dict()\n",
    "        with open(test_img_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line not in test_path:\n",
    "                    test_path[line] = 1\n",
    "        # print(test_path)\n",
    "\n",
    "        with open(annot_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                key, caption = line[0].split('#')[0], line[1]\n",
    "                # NOTE 只保留test set的\n",
    "                if key not in test_path:\n",
    "                    continue\n",
    "                # if line[0].split('#')[-1] != '0':\n",
    "                #     # print(key, line[0].split('#')[-1])\n",
    "                #     continue # 只保留一句\n",
    "                img_path = key + '.jpg'\n",
    "\n",
    "                if 'train' in img_path:\n",
    "                    self.images.append(os.path.join('train2014' ,img_path) )\n",
    "                else:\n",
    "                    self.images.append(os.path.join('val2014' ,img_path) )\n",
    "                self.captions.append(caption)\n",
    "                self.labels.append(key)\n",
    "        self.transforms = transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "        # NOTE large 模型\n",
    "        self.context_length = 77\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.images[idx])\n",
    "        image = self.transforms(Image.open( os.path.join(self.root, img_path ))) \n",
    "        text = self.tokenizer(str(self.captions[idx]), max_length=self.context_length, padding='max_length', truncation=True, return_tensors='pt')['input_ids'][0]\n",
    "        label = self.labels[idx]\n",
    "        return image, text, label\n",
    "\n",
    "from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, \\\n",
    "    CenterCrop\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "def image_transform(\n",
    "        image_size: int,\n",
    "        is_train: bool,\n",
    "        mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "        std=(0.26862954, 0.26130258, 0.27577711)\n",
    "):\n",
    "    normalize = Normalize(mean=mean, std=std)\n",
    "    if is_train:\n",
    "        return Compose([\n",
    "            RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "            _convert_to_rgb,\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            _convert_to_rgb,\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "val_transform = image_transform(224, False)\n",
    "dataset = COCO_CN(transform = val_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1053, 512]) torch.Size([1053, 512]) 1053\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import CLIPModel\n",
    "import torch\n",
    "# NOTE load model\n",
    "\n",
    "text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\").cuda().eval()\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval() \n",
    "\n",
    "# text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\").cuda().eval()\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").cuda().eval() \n",
    "\n",
    "\n",
    "all_img_features, all_text_features, all_labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader):\n",
    "        images, captions, labels = data\n",
    "        images = images.cuda()\n",
    "        captions = captions.cuda()\n",
    "        all_labels.extend(labels)\n",
    "        # print(images.shape, captions.shape, labels)\n",
    "\n",
    "        image_features = clip_model.get_image_features(images)\n",
    "        text_features = text_encoder(captions).logits\n",
    "        # 归一化\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        all_img_features.append(image_features)\n",
    "        all_text_features.append(text_features)\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "    img_features = torch.cat(all_img_features)\n",
    "    text_features = torch.cat(all_text_features)\n",
    "    print(img_features.shape, text_features.shape, len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text r1:0.5289648622981956, r5:0.8110161443494777, r10:0.8983855650522318\n",
      "text_to_image r1:0.4624881291547958, r5:0.7806267806267806, r10:0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "get_metrics(img_features, text_features, all_labels, 100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flickr30k-CNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "class flickr30k_CNA(Dataset):\n",
    "    def __init__(self, img_root_path='/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/flickr30k/images', \\\n",
    "                text_annot_path='/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/test/flickr30k_cn_test.txt', \\\n",
    "                transform=None):\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self.labels = []\n",
    "        self.root = img_root_path\n",
    "        with open(text_annot_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                key, caption = line[0].split('#')[0], line[1]\n",
    "                img_path = key + '.jpg'\n",
    "                self.images.append(img_path)\n",
    "                self.captions.append(caption)\n",
    "                self.labels.append(key)\n",
    "        self.transforms = transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "        # NOTE large 模型\n",
    "        self.context_length = 77\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.images[idx])\n",
    "        image = self.transforms(Image.open( os.path.join(self.root, img_path ))) \n",
    "        text = self.tokenizer(str(self.captions[idx]), max_length=self.context_length, padding='max_length', truncation=True, return_tensors='pt')['input_ids'][0]\n",
    "        label = self.labels[idx]\n",
    "        return image, text, label\n",
    "\n",
    "from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, \\\n",
    "    CenterCrop\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "def image_transform(\n",
    "        image_size: int,\n",
    "        is_train: bool,\n",
    "        mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "        std=(0.26862954, 0.26130258, 0.27577711)\n",
    "):\n",
    "    normalize = Normalize(mean=mean, std=std)\n",
    "    if is_train:\n",
    "        return Compose([\n",
    "            RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "            _convert_to_rgb,\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        return Compose([\n",
    "            Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            _convert_to_rgb,\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "val_transform = image_transform(224, False)\n",
    "img_root = '/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/flickr30k/images'\n",
    "text_annot_path = '/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/test/flickr30k_cn_test.txt'\n",
    "dataset = flickr30k_CNA(img_root, text_annot_path, val_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import CLIPModel\n",
    "import torch\n",
    "# text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\").cuda().eval()\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval() \n",
    "\n",
    "text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\").cuda().eval()\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").cuda().eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 768]) torch.Size([5000, 768]) 5000\n"
     ]
    }
   ],
   "source": [
    "all_img_features, all_text_features, all_labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader):\n",
    "        images, captions, labels = data\n",
    "        images = images.cuda()\n",
    "        captions = captions.cuda()\n",
    "        all_labels.extend(labels)\n",
    "        # print(images.shape, captions.shape, labels)\n",
    "\n",
    "        image_features = clip_model.get_image_features(images)\n",
    "        text_features = text_encoder(captions).logits\n",
    "        # 归一化\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        all_img_features.append(image_features)\n",
    "        all_text_features.append(text_features)\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "    img_features = torch.cat(all_img_features)\n",
    "    text_features = torch.cat(all_text_features)\n",
    "    print(img_features.shape, text_features.shape, len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_to_text r1:0.659, r5:0.903, r10:0.957\n",
      "text_to_image r1:0.5108, r5:0.782, r10:0.8594\n"
     ]
    }
   ],
   "source": [
    "get_metrics(img_features, text_features, all_labels, 100)  # 图片取前1000张，因为后面的是重复的（每张图片对应5个caption）Flickr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a2872ef89789832e0a654d6c95a175dab3d7e4133113b4cef309e372e0ba06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
